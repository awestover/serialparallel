\section{An Algorithm for Optimizing Awake Time} 
\label{sec:awaketime}
In this section we give a simple deterministic scheduling
algorithm --- that does not use preemption --- for minimizing
awake time; note that we only consider the metric of awake time
in this section. We show that our algorithm is $2$-competitive
with OPT (in terms of awake time) on all TAPs.

Note that without loss of generality we may consider TAPs where
the cost ratio $\pi(\tau)/\sigma(\tau) \in [1,p]$ for all tasks
$\tau$; if $\pi(\tau)/\sigma(\tau) < 1$ then the scheduler
clearly should never run $\tau$ in serial so we can replace the
serial implementation with the parallel implementation to get
cost ratio $1$, similarly, if $\pi(\tau)/\sigma(\tau) > p$ then
the scheduler should never run $\tau$ in parallel and we can
replace the parallel implementation with the serial
implementation to get cost ratio $p$.

We say that a time is a \defn{verge} time for our algorithm if at
this time no processors have work assigned to them, and there is at
least one ready task.

We propose Algorithm~\ref{alg:gr}, which we call \defn{GR}
(GR is short for \enquote{greedy}), as a scheduling algorithm.

\begin{algorithm}
  \caption{GR}
  \label{alg:gr}
  \begin{algorithmic}
    \While{True}
      \If{verge time}
        \State schedule tasks as directed by ORACLE-1 
      \EndIf
    \EndWhile
  \end{algorithmic}
\end{algorithm}

A \defn{single-time-TAP} is a TAP where all tasks arrive at a
single time. ORACLE-1 is an algorithm that yields a schedule
achieving minimal awake time, i.e. the same awake time as OPT,
on single-time-TAPs. In Lemma~\ref{lem:frosting} we establish
that ORACLE-1, which we specify in
Algorithm~\ref{alg:opt_oracle}, is an oracle for OPT on
single-time-TAPs, hence showing that GR can actually be computed.
We remark that it is not obvious that an oracle for OPT can be
computed in finite time, even in this special case: there are an
uncountably infinite number of possible scheduling strategies
that OPT can choose from. Nevertheless, we show how to consider a
finite search space that a search can actually be performed on,
at least in the special case of single-time-TAPs. Before
analyzing ORACLE-1 we analyze the competitive ratio of GR
assuming the correctness of ORACLE-1. 

Consider a TAP $\mathcal{T}$. Let $\ell$ be the number of verge
times for $\mathcal{T}$; note that $\ell \le n$ which in
particular is finite. Let $t_i$ be the $i$-th time that is a
verge time, let $q_i$ be the number of ready tasks for GR at
time $t_i$. Let $T^{ALG}(q_1, \ldots, q_{\ell'})$ denote the
awake time of a scheduling algorithm ALG on the truncation of the
TAP $\mathcal{T}$ that only consists of tasks arriving at times
before $t_{\ell'}$.

By construction of ORACLE-1 we have that for single-time-TAPs OPT
and GR achieve the same awake time, i.e.
\begin{equation}
  \label{eq:same_single}
  T^{OPT}(q) = T^{GR}(q).
\end{equation}
We remark GR \enquote{locally} schedules optimally, which is why
we refer to GR as \enquote{greedy}. 

An \defn{ALG-gap} is an interval of time $I$ of non-zero length where for
all times in the interior of $I$ ALG has completed every
task that has arrived thus far. Additionally, for an interval to
be an ALG-gap the interval must contain no other intervals which
are also ALG-gaps (i.e. it is a \enquote{maximal} interval
satisfying our conditions).
We say that a TAP is \defn{ALG-gap-free} if it contains no ALG-gaps.

Now we prove an obvious property of OPT.
\begin{claim}
  \label{clm:OPT_finishes_first}
  If there is a scheduling algorithm ALG that completes all tasks by
  time $t_*$ then OPT finishes all tasks by time $t_*$.
\end{claim}
\begin{proof}
  Say that ALG completes all tasks by time $t_*$. Let $t_0 < t_*$
  be the most recent time that OPT has completed all tasks that
  arrive before time $t_0$. If OPT has not finished all tasks by
  time $t_*$ then it was acting sub-optimally, as it could steal
  the strategy that ALG used on $[t_0, t_*]$ to achieve lower
  awake time. In particular, for any tasks that arrive in $[t_0,
  t_*]$ OPT could schedule them as ALG schedules them. 
\end{proof}
As an immediate consequence of Claim~\ref{clm:OPT_finishes_first}
we have that any ALG-gap is a subset of an OPT-gap.

Decomposing TAPs into gap-free subsets of the TAP is very useful.
Part of the reason for this is the following fact:
\begin{claim}
  \label{clm:just_consider_gapless}
  If an algorithm ALG achieves competitive ratio $r$ on
  ALG-gap-free TAPs, then ALG achieves 
  competitive ratio $r$ on arbitrary TAPs.
\end{claim}
\begin{proof}
  We partition the tasks based on arrival time, splitting the
  tasks on the ALG-gaps. That is, we split the tasks into groups
  so that two tasks $\tau_i, \tau_j$ are in the same group if and
  only if there are no gaps in between the arrival times of
  $\tau_i$ and $\tau_j$.
  We can define an interval of time $I_i$ for each of these
  ALG-gap-free subsets of the TAP, where $I_i$ is defined so that
  all tasks in the $i$-th group start and finish at times
  contained in the interval $I_i$.

  Let $T_{I_i}^{OPT}$ and $T_{I_i}^{ALG}$ denote the awake time
  of OPT and ALG on interval $I_i$. Because $I_i$ is ALG-gap-free
  we have $T^{ALG} = \sum_{i} T^{ALG}_{I_i}$.
  Further, recall that by Claim~\ref{clm:OPT_finishes_first} any
  ALG-gap is also an OPT-gap, so
  $T^{OPT} = \sum_{i} T_{I_i}^{OPT}$. 
  Hence from our assumption that ALG is $r$-competitive on
  gap-free TAPs, such as the subset of the TAP on the interval
  $I_i$, we have $T_{I_i}^{ALG} \le r\cdot T_{I_i}^{OPT}$ for
  all $i$. Summing we get $T^{ALG} \le r\cdot T^{OPT}$, as desired.
  
\end{proof}

By Claim~\ref{clm:just_consider_gapless}, in order to bound GR's
competitive ratio, it suffices to consider TAPs
without GR-gaps. Note however that a TAP without
GR-gaps could still have OPT-gaps.

We conclude our analysis of the competitive ratio of GR in
Proposition~\ref{prop:2competitive} with an inductive argument on
the number of OPT-gaps in the TAP.
First we establish the base case for the argument in
Claim~\ref{clm:no_optgaps}: we consider
GR's competitive ratio on an OPT-gap-free TAP.  

\begin{claim}
  \label{clm:no_optgaps}
  GR is $2$-competitive on OPT-gap-free TAPs.
\end{claim}
\begin{proof}
  For an OPT-gap-free TAP we must have
  \begin{equation}
    \label{eq:opt_isnt_so_much_better}
    T^{OPT}(q_1, \ldots, q_{\ell}) \ge T^{GR}(q_1, \ldots, q_{\ell-1}).
  \end{equation}
  Because GR finishes all $q_{i}$ tasks that arrive at time $t_i$
  by time $t_{i+1}$ we can actually always decompose
  $T^{GR}(q_1, \ldots, q_\ell)$ as 
  \begin{equation}
    \label{eq:decomposeGR}
    T^{GR}(q_1, \ldots, q_\ell) = \sum_{i=1}^\ell T^{GR}(q_i).
  \end{equation}
  By Equation~\eqref{eq:decomposeGR}, and
  Equation~\eqref{eq:same_single} we thus have 
  \begin{equation}
    \label{eq:decompose_rearanged}
    T^{GR}(q_1, \ldots, q_\ell) = T^{GR}(q_1, \ldots, q_{\ell-1}) + T^{OPT}(q_\ell).
  \end{equation}

  Hence by Equation~\eqref{eq:opt_isnt_so_much_better} and
  Equation~\eqref{eq:decompose_rearanged} we have
  \begin{align*}
    T^{GR}(q_1, \ldots, q_\ell) &\le T^{OPT}(q_1, \ldots, q_\ell) + T^{OPT}(q_\ell)\\
                                   &\le 2T^{OPT}(q_1, \ldots, q_\ell),
  \end{align*}
  as desired.
\end{proof}

\begin{proposition}
  \label{prop:2competitive}
  GR is $2$-competitive.
\end{proposition}
\begin{proof}
  The proof is by strong induction on the number of OPT-gaps. 
  The base case of our induction is established in
  Claim~\ref{clm:no_optgaps}, which says that if there are $0$
  OPT-gaps then GR is $2$-competitive. 

  Consider a TAP that has more than $0$ OPT gaps;
  say that its first OPT-gap starts at time $t_*$.
  Let $j$ be the largest index such that verge time $t_j <
  t_*$.

  Using our inductive hypothesis we have:
  \begin{align*}
  &T^{OPT}(q_1, \ldots, q_\ell) \\
  &\ge T^{OPT}(q_1, \ldots, q_j) + T^{OPT}(q_{j+1}, \ldots, q_{\ell})\\
  &\ge \frac{1}{2}\paren{T^{GR}(q_1, \ldots, q_j) + T^{GR}(q_{j+1}, \ldots, q_{\ell})}\\
  &=\frac{1}{2} T^{GR}(q_1, \ldots, q_\ell).
  \end{align*}

\end{proof}

Now we analyze Algorithm~\ref{alg:opt_oracle}, which we call
ORACLE-1. 

\begin{algorithm}
  \caption{ORACLE-1}
  \label{alg:opt_oracle}
  \begin{algorithmic}
    \State \textbf{Input:} tasks $\tau_1,\ldots, \tau_n$ all with $t(\tau_i) = 0$
    \State \textbf{Output:} a way to schedule the tasks to
    processors $\rho_1, \ldots, \rho_p$ that achieves minimal awake time
    \State 
    \State $\text{minAwakeTime} \gets \infty$
    \State $\text{bestSchedule} \gets $ schedule everything in serial on $\rho_1$
    \For{$I \in \{0,1\}^n$} 
      \State $x \gets \sum_{i=1}^n I_i$
      \For{$J \in \{1, \ldots, p\}^x$}
        \State $j \gets 0$
        \For{$i \in \{1,2,\ldots, n\}$}
          \If{$I_i=1$}
            \State $j \gets j+1$
            \State schedule task $\tau_i$ in serial on $\rho_{J_j}$
          \EndIf
        \EndFor
        \State $m \gets \max_{\rho_i}(\work(\rho_i))$
        \State $w \gets \sum_{\rho_i} \paren{m - \work(\rho_i)}$
        \State $f \gets \sum_{\rho_i} (1-I_i)\pi(\rho_i)$
        \If{$f \ge w$}
          \State make $\rho_i$ have work $m + (f-w)/p$
        \Else 
          \State distribute $f$ units of work arbitrarily 
          \State among $\rho_i$ without increasing awake time
        \EndIf
        \If{$\text{awakeTime}(I, J) \le \text{minAwakeTime}$}
          \State{$\text{minAwakeTime} \gets \text{awakeTime}(I, J)$}
          \State{$\text{bestSchedule} \gets \text{schedule}(I, J)$}
        \EndIf
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

The key insight to decrease the search space to be finite is to
notice that for any method of distributing whichever tasks are
chosen to run in serial, the parallel tasks may as well be
redistributed afterwords, so long as doing so either results in
not increasing the awake time, or results in all tasks having
identical amounts of work.
We can thus do a brute-force search over all the ways to assign some
tasks to run in serial and to run on specific processors, and
then put the parallel tasks on top \enquote{like frosting on a cake}. 

We remark that the running time of ORACLE-1 for $n$ tasks is much
larger than $(2p)^n$ which is extremely large. Nevertheless the
existence of our algorithm is interesting; we reduce the running
time of our algorithm later.

We now prove that ORACLE-1 actually does compute a schedule with
awake time the same as that of OPT.
\begin{lemma}[Frosting Lemma]
  \label{lem:frosting} 
  ORACLE-1 is an oracle for OPT on TAPs where all tasks arrive at a single time.
\end{lemma}
\begin{proof}
  Consider the configuration that OPT chooses. ORACLE-1 considers
  a configuration of tasks with the same assignment of serial
  tasks at some point, because ORACLE-1 brute force searches
  through all of these. For this configuration it is clearly
  impossible to achieve lower awake time than by spreading the
  parallel tasks in the frosting method, hence OPT's awake time
  is at least that of ORACLE-1.
\end{proof}

This existence proof is nice. We now consider how to
more efficiently compute the best schedule for the case where all
tasks arrive at the start. In fact, we consider making an
algorithm to get a constant approximation of this, which will
still give an algorithm with constant competitive ratio.

We propose Algorithm~\ref{alg:thresh}, which we call THRESH, as a
simple way to $2$-approximate ORACLE-1.

\begin{algorithm}
  \caption{THRESH}
  \label{alg:thresh}
  \begin{algorithmic}
    \State \textbf{Input:} tasks $\tau_1,\ldots, \tau_n$ with
    $\sigma(\tau_1) \ge \sigma(\tau_2)\ge \cdots \ge
    \sigma(\tau_n)$ all with $t(\tau_i) = 0$
    \State \textbf{Output:} a way to schedule the tasks to
    processors $\rho_1, \ldots, \rho_p$ that achieves awake time
    at most $3$ times the optimal awake time
    \State
    \State $w_\sigma \gets \sum_i \sigma(\tau_i)$
    \State $w_\pi \gets 0$
    \State $a \gets 0$, $i_* = 0$
    \For{$i \in [n]$}
      \State $w_\sigma \gets w_\sigma - \sigma(\tau_i)$
      \State $w_\pi \gets w_\pi + \pi(\tau_i)$
      \If{$i < n$}
        \State $a_0 = w_\sigma / p + \sigma(\tau_{i+1}) + w_\pi/p$
      \Else
        \State $a_0 = w_\pi/p$
      \EndIf
      \If{$a_0 \le a$}
        \State $i_* \gets i$
        \State $a \gets a_0$
      \EndIf
    \EndFor
    \State Schedule tasks $\tau_1, \ldots, \tau_{i_*}$ in
    parallel, distributing their work equally among the
    processors.
    \State $j \gets 0$
    \For{$k \in \{i_* + 1, \ldots, n\} $}
    \State Schedule $\tau_{k}$ in serial on processor $\rho_{1+(j \bmod p)}$
    \EndFor
  \end{algorithmic}
\end{algorithm}

Put simply THRESH schedules the $i_*$ tasks with largest serial
work in parallel, distributing their work equally, and schedules
the rest of the tasks in serial, sequentially giving out the
tasks, for the optimal value of $i_*$.

Clearly THRESH requires space $\Theta(1)$ beyond the input to
implement, and has running time $\Theta(n)$, given that the input
is pre-sorted.

In order to remove the assumption that the input is pre-sorted,
we can sort the tasks at the start of the algorithm, using heap
sort or radix sort. If we simply use heap sort then we clearly
will have running time $\Theta(n \log n)$ but still have $O(1)$
space requirement, and have competitive ratio $3$ with ORACLE-1.
On the other hand, if we use radix sort, it is conceivable that
we could do better. First, if the tasks can be assumed to have
serial works that can only take on $2^{O(1)}$ possible values,
then the radix sort can be performed in linear time, in-place,
resulting in a $2$-competitive algorithm that uses $O(1)$ memory
and $O(n)$ time. On the other hand, if such an assumption cannot
be made, then if better running time is still desired, then it
can be achieved at a cost to the competitive-ratio. Let $1$ be
the largest serial work of any task. If we sort the tasks based
on the key of which bucket $[1/2^i, 1/2^{i-1}]$ the task's
serial work falls in, except saying
that any tasks with work less than $1/2^{\lg (np)}$ fall in the
same bucket, then there are only $\log n + \log p$ distinct keys,
so sorting can be done much faster. If we are willing to spend
$\Theta(n + \log (np))$ space, then the sorting can be done by
counting sort in time $\Theta(n+\log(np))$. On the other hand,
using radix sort we could do the sort in time $\Theta(n \log
(np))$ using only $\Theta(1)$ auxiliary space. 
Since the tasks are only approximately sorted we can only
guarantee $4$-competitiveness in this case: this is clear by
noting that increasing the serial works of all tasks by a factor
of $2$ would double the best attainable awake time.

Now we show why THRESH is $2$-competitive.
Consider OPT's strategy.
Let $\tau_{i_0}$ be the task with the largest serial work that
OPT schedules in serial. Recalling that the tasks are sorted by
serial work, for all $i < i_0$ we have that OPT chooses to
schedule $\tau_i$ in parallel. Let $w^{OPT}_\sigma = \sum_{i \ge
i_0} \sigma(\tau_i), w^{OPT}_\pi = \sum_{i < i_0} \pi(\tau_i)$.
OPT's awake time is obviously at least $(w^{OPT}_\pi +
w^{OPT}_\sigma)/p$. OPT's awake time is also obviously at least
$\sigma(\tau_{i_0})$.

Think about the sequential thing.
Ignore $\sigma(\tau_{i_*})$. Then $\rho_i$ always has less work
than all the other processors. Of course in reality $\rho_1$ has
the most work of any processor.
So we have that all processors have serial work at most 
$$\sum_{i > i_*} \sigma(\tau_i)/p + \sigma(\tau_{i_*}).$$
Add on to that $$\sum_{i \le i_*} \pi(\tau_i)/p.$$

If $x \le a + b$, and $y \ge a, y \ge b$, then obviously $x \le
2y$. Thus, we're $2$-competitive. 


\section{Minimizing Awake Time on TAPs} 
\label{sec:awaketime}
In this section we show that the on-line scheduling problem can
essentially be reduced to the off-line \defn{single-arrival-time}
case of the problem: the case where all tasks arrive at the
start. In particular, we give an extremely simple deterministic
scheduling algorithm, that in particular does not use preemption,
for minimizing awake time, that makes use of an algorithm for the
batch case of the off-line problem. If we use an algorithm that
is $R$-competitive with OPT in the off-line batch case in our
algorithm, then we get an on-line algorithm that is
$2R$-competitive with OPT. We give an inefficient algorithm that
accomplishes $R=1$, and several more interesting and efficient
algorithms with larger values of $R$.

\subsection{On-line Reduction to Off-line}
A \defn{single-time TAP} is a TAP where all tasks arrive at the
same time. In this subsection we show that algorithms that are
$R$-competitive with OPT on single-time TAPs can be used to make
an on-line algorithm that is $2R$-competitive with OPT.

Without loss of generality we consider TAPs where the cost ratio
$\pi(\tau)/\sigma(\tau) \in [1,p]$ for all tasks $\tau$; if
$\pi(\tau)/\sigma(\tau) < 1$ then the scheduler clearly should
never run $\tau$ in serial so we can replace the serial
implementation with the parallel implementation to get cost ratio
$1$, similarly, if $\pi(\tau)/\sigma(\tau) > p$ then the
scheduler should never run $\tau$ in parallel and we can replace
the parallel implementation with the serial implementation to get
cost ratio $p$.

A \defn{verge} time for our algorithm is a time where no
processors have work assigned to them, and there is at least one
ready task. We propose Algorithm~\ref{alg:bat}, which we call
\defn{$\bat_R$} (BAT is short for \enquote{batch}), as a
scheduling algorithm for the on-line problem. In this subsection
we analyze the competitive-ratio of $\bat_R$ assuming the
existence of an algorithm for $\oracle_R$; in
Subsection~\ref{subsec:offlineawaketime} we show that $\oracle_R$
can be computed, and in fact can be efficiently computed for $R \ge 2$.

\begin{algorithm}
  \caption{$\bat_R$}
  \label{alg:bat}
  \begin{algorithmic}
    \State Let $\oracle_R$ be an algorithm that is $R$-competitive with OPT on single-time TAPs 
    \If{verge time}
      \State schedule tasks as directed by $\oracle_R$
    \EndIf
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
  \label{thm:constant_competitive}
  $\bat_R$ is $2R$-competitive.
\end{theorem}
\begin{proof}
  Let an \defn{OPT-batch} be a maximal set of tasks done by OPT.
  Let a \defn{BAT-batch} be one of $\bat_R$'s batches. Call a BAT-batch
  \defn{internal} if it is a subset of an OPT-batch. Call a
  BAT-batch \defn{external} if it is not a subset of an
  OPT-batch. 

  Consider an OPT-batch $x$. Let $B_1$ be the union of the
  internal BAT-batches contained in $x$, let $B_2$ be the
  BAT-batch that overlaps with the end of $x$ (if such a batch
  exists), and let $B_3$ be the first BAT-batch that starts after
  $x$. Let $T_1$ be the time since the start of $x$ to the start
  of $B_2$ and let $T_2$ be the time from the start of $B_2$
  until the end of $x$. The amount of time that OPT spends on
  tasks from $x$ is clearly $T_1+T_2$. Now we bound how much time
  BAT spends on tasks from $x$. $B_1$ takes time $T_1$. $B_3$
  spends time at most $RT_2$ on tasks from $x$. $B_2$ spends time
  at most $R(T_1 + T_2)$ on tasks from $x$. Adding these times up
  we get that BAT spends at most $2R(T_1+T_2)$ time on the tasks
  from $x$. Adding this up across all OPT-batches $x$, gives that
  BAT is $2R$-competitive, as desired.
\end{proof}

In fact, if we assume no properties of $\oracle_R$ beyond that it
always gives a schedule that is $R$-competitive with OPT on
single-time TAPs, the $2R$ competitive ratio is tight.
\begin{proposition}
  \label{prop:thm_is_tight_ish}
  Let $R < p/2$. Fix $\varepsilon > 0$.
  There exists an $\oracle_R$ such that $\bat_R$ using this
  $\oracle_R$ does not get competitive ratio $(2R-\varepsilon)$
  on some TAP.
\end{proposition}
\begin{proof}
  Consider an $\oracle_R$ that, if it gets $R$ tasks with serial
  work $1$ and parallel work $p$, schedules the tasks one after
  another on $\rho_1$. Clearly this is $R$-competitive with OPT:
  OPT takes time $1$, while $\oracle_R$ takes time
  $R$. Now consider a TAP where at time $0$ and at time
  $\varepsilon$ a set of $R$ tasks with serial work $1$ and
  parallel work $p$ arrive. Clearly OPT achieves awake time
  $1+\varepsilon$ on this TAP, while $\bat_R$, using the
  specified $\oracle_R$, achieves awake time $2R$.
  Letting $\varepsilon' = \frac{\varepsilon}{1+\varepsilon}$ we
  see that the competitive ratio of $\bat_R$ on this TAP is
  $2R-\varepsilon'$. Letting $\varepsilon'' = \varepsilon/2$, we
  have that $\bat_R$ does not get competitive ratio
  $2R-\varepsilon''$ on this TAP.
\end{proof}
We remark that Proposition~\ref{prop:thm_is_tight_ish} does not
imply that there is no $\oracle_R$ such that $\bat_R$ is
$(2R-\varepsilon)$-competitive for some $\varepsilon > 0$ on all
TAPs. But it does imply that we would need extra assumptions on
$\oracle_R$ to get such a result. It also means that our
reduction transformation is tight.

\subsection{Off-line Approximation Algorithms}
\label{subsec:offlineawaketime}
In this Subsection we present algorithms for $\oracle_R$ for
various values of $R$.

First we give Algorithm~\ref{alg:frost}, which we call
FROST, for $\oracle_1$. 

\begin{algorithm}
  \caption{FROST (i.e. $\oracle_1$)}
  \label{alg:frost}
  \begin{algorithmic}
    \State \textbf{Input:} tasks $\tau_1,\ldots, \tau_n$ all with $t(\tau_i) = 0$
    \State \textbf{Output:} a way to schedule the tasks to
    processors $\rho_1, \ldots, \rho_p$ that achieves minimal awake time
    \State 
    \State $\text{minAwakeTime} \gets \infty$
    \State $\text{bestSchedule} \gets $ schedule everything in serial on $\rho_1$
    \For{$I \in \{0,1\}^n$} 
      \State $x \gets \sum_{i=1}^n I_i$
      \For{$J \in \{1, \ldots, p\}^x$}
        \State $j \gets 0$
        \For{$i \in \{1,2,\ldots, n\}$}
          \If{$I_i=1$}
            \State $j \gets j+1$
            \State schedule task $\tau_i$ in serial on $\rho_{J_j}$
          \EndIf
        \EndFor
        \State $m \gets \max_{\rho_i}(\work(\rho_i))$
        \State $w \gets \sum_{\rho_i} \paren{m - \work(\rho_i)}$
        \State $f \gets \sum_{\rho_i} (1-I_i)\pi(\rho_i)$
        \If{$f \ge w$}
          \State make $\rho_i$ have work $m + (f-w)/p$
        \Else 
          \State distribute $f$ units of work arbitrarily 
          \State among $\rho_i$ without increasing awake time
        \EndIf
        \If{$\text{awakeTime}(I, J) \le \text{minAwakeTime}$}
          \State{$\text{minAwakeTime} \gets \text{awakeTime}(I, J)$}
          \State{$\text{bestSchedule} \gets \text{schedule}(I, J)$}
        \EndIf
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

We remark that it is not obvious that an oracle for OPT can be
computed in finite time, even in the single-arrival-time case:
there are an uncountably infinite number of possible scheduling
strategies that OPT can choose from. The key insight to decrease
the search space to be finite is to notice that for any method of
distributing whichever tasks are chosen to run in serial, the
parallel tasks may as well be redistributed afterwords, so long
as doing so either results in not increasing the awake time, or
results in all tasks having identical amounts of work. We can
thus do a brute-force search over all the ways to assign some
tasks to run in serial and to run on specific processors, and
then put the parallel tasks on top \enquote{like frosting on a
cake}. We remark that the running time of FROST for $n$ tasks is
$\Theta(p^n)$, which is extremely large. Nevertheless the
existence of the algorithm is interesting. 

Now we prove that FROST is $1$-competitive with OPT.
\begin{lemma}[Frosting Lemma]
  \label{lem:frosting} 
  FROST is an oracle for OPT on single-time TAPs.
\end{lemma}
\begin{proof}
  Consider the configuration that OPT chooses. FROST considers
  a configuration of tasks with the same assignment of serial
  tasks at some point, because FROST brute force searches through
  all such configurations. For this configuration it is clearly
  impossible to achieve lower awake time than by spreading the
  parallel tasks in the frosting method, hence OPT's awake time
  is at least that of FROST.
\end{proof}

We remark that it is straightforward to extend FROST to a full
oracle for OPT. In particular, a full OPT oracle can be
constructed as follows: 

\begin{enumerate}
  \item Perform a brute force search over every
possible subset of the tasks to be the set of tasks scheduled in
parallel, and do a brute force search over every way to assign
the serial tasks to processors. 

\item Now consider the gaps, i.e. times
when all processors are idle under this current schedule. Add the
parallel tasks, scheduling them so as not to decrease the size of
any gaps unless it is impossible to do otherwise, in which case
the parallel tasks should be added to make all processors have
equal amounts of work extending into what used to be a gap.
\end{enumerate}

We now consider how to efficiently approximate OPT on
single-time TAPs. We propose Algorithm~\ref{alg:thresh}, which we
call THRESH, as a simple way to $2$-approximate OPT.

\begin{algorithm}
  \caption{THRESH}
  \label{alg:thresh}
  \begin{algorithmic}
    \State \textbf{Input:} tasks $\tau_1,\ldots, \tau_n$ with
    $\sigma(\tau_1) \ge \sigma(\tau_2)\ge \cdots \ge
    \sigma(\tau_n)$ all with $t(\tau_i) = 0$
    \State \textbf{Output:} a way to schedule the tasks to
    processors $\rho_1, \ldots, \rho_p$ that achieves awake time
    at most twice the optimal awake time.
    \State
    \State $w_\sigma \gets \sum_i \sigma(\tau_i)$
    \State $w_\pi \gets 0$
    \State $a \gets 0$, $i_* = 0$
    \For{$i \in [n]$}
      \State $w_\sigma \gets w_\sigma - \sigma(\tau_i)$
      \State $w_\pi \gets w_\pi + \pi(\tau_i)$
      \If{$i < n$}
        \State $a_0 = w_\sigma / p + \sigma(\tau_{i+1}) + w_\pi/p$
      \Else
        \State $a_0 = w_\pi/p$
      \EndIf
      \If{$a_0 \le a$}
        \State $i_* \gets i$
        \State $a \gets a_0$
      \EndIf
    \EndFor
    \State Schedule tasks $\tau_1, \ldots, \tau_{i_*}$ in
    parallel, distributing their work equally among the
    processors.
    \For{$k \in \{i_* + 1, \ldots, n\} $}
    \State Schedule $\tau_{k}$ in serial on processor $\rho_{1+(k \bmod p)}$
    \EndFor
  \end{algorithmic}
\end{algorithm}

THRESH schedules the $i_*$ tasks with largest serial work in
parallel, distributing their work equally, and schedules the rest
of the tasks in serial, sequentially giving out the tasks, for
the optimal value of $i_*$. Clearly THRESH only requires space
$\Theta(1)$ beyond the input to implement, and has running time
$\Theta(n)$, given that the input is pre-sorted.
We remark that a nice feature of THRESH is that it schedules
parallel tasks on a constant number of processors; we allow OPT
to schedule tasks on a variable number of processors (i.e.
unevenly distribute the work of a parallel task). It is nice to
not need to be able to unevenly distribute parallel tasks.

\begin{proposition}
THRESH is $2$-competitive with OPT on single-time TAPs.
\end{proposition}
\begin{proof}
Let $\tau_{i_0}$ be the task with the largest serial work that
OPT schedules in serial. Recalling that the tasks are sorted by
serial work, for all $i < i_0$ we have that OPT chooses to
schedule $\tau_i$ in parallel. Let $$w^{OPT}_\sigma = \sum_{i \ge
i_0} \sigma(\tau_i), w^{OPT}_\pi = \sum_{i < i_0} \pi(\tau_i).$$
OPT's awake time is obviously at least $$(w^{OPT}_\pi +
w^{OPT}_\sigma)/p.$$ OPT's awake time is also obviously at least
$\sigma(\tau_{i_0})$.

Without loss of generality we call the processor that gets the
most work, which used to be called $\rho_{1+(i_* \bmod p)}$,
$\rho_1$; in particular we circularly shift the labels of the
processors so that the work that $\rho_i$ gets is larger than the
work that $\rho_j$ gets for any $j > i$.
Think about the sequential thing.
Ignore $\sigma(\tau_{i_*})$. Then $\rho_i$ always has less work
than all the other processors. Of course in reality $\rho_1$ has
the most work of any processor.
So we have that all processors have serial work at most 
$$\sum_{i > i_*} \sigma(\tau_i)/p + \sigma(\tau_{i_*}).$$
Add on to that $$\sum_{i \le i_*} \pi(\tau_i)/p.$$

If $x \le a + b$, and $y \ge a, y \ge b$, then obviously $x \le
2y$. Thus, THRESH is $2$-competitive with FROST. 
\end{proof}

We now remark on the competitive-ratio of $\bat_2$ using THRESH.
By Theorem~\ref{thm:constant_competitive} THRESH achieves
competitive ratio at most $4$ on all TAPs. However,
Proposition~\ref{prop:thm_is_tight_ish} \emph{does not} imply
that THRESH must not be $3$-competitive. In fact, we believe the
opposite:
\begin{conjecture}
  $\bat_2$ using THRESH is $3$-competitive with OPT.
\end{conjecture}

The competitive ratio of $\bat_2$ using THRESH is not (much) better than $3$:
\begin{proposition}
  There exists a TAP such that $\bat_2$ using THRESH has
  competitive ratio larger than $3-3/p$.
\end{proposition}
\begin{proof}
  Consider the following TAP: at time $0$ a completely unscalable
  task with serial work $1$ arrives along with a fully scalable
  task with parallel work $p-2$, and then at time $1/(p-1)$ a
  completely unscalable task with serial work $1$ arrives.
  $\bat_2$ using THRESH takes time $(p-2)/p + 1 + 1 = 3-2/p$ to
  complete these tasks, whereas OPT takes time $1+1/(p-1)$.
  The competitive ratio of $\bat_2$ here is thus
  $$\frac{3-2/p}{1+1/(p-1)} = (3-2/p)(1-1/p) > 3-3/p.$$
\end{proof}
However, we have not been able to find a TAP where $\bat_2$ using
THRESH exhibits competitive ratio larger than $3$, hence our
conjecture. 

The assumption that the input is pre-sorted might not be
reasonable. If the tasks are not pre-sorted, then we must (at
least approximately) sort them for our algorithm to achieve a
good competitive-ratio. We now discuss several approaches for
doing this sorting which will involve trade-offs in competitive
ratio, running time, and space.

If we simply use heap sort to sort the tasks, then we
will have increased running time of $\Theta(n \log n)$, but will
retain our constant-space requirement and will still have
competitive ratio $2$.

If we use radix sort then we can possibly achieve better
running time. 

If the tasks can be assumed to
have serial works that can only take on $O(1)$ possible
values, then the radix sort can be performed in-place in linear
time, which results in a linear-time $2$-competitive algorithm. 

If the serial works can take on more than $\omega(1)$ possible
values, we can still achieve better than linearithmic running
time by sacrificing competitive ratio. Let $1$ be the largest
serial work of any task. We sort the tasks based on which bucket
$[1/2^i, 1/2^{i-1}]$ the task's serial work falls in, --- i.e.
rounding their serial works ---  except putting tasks with work
less than $1/2^{\lg (np)}$ in a single bucket. Now there are only
$\log (np)$ distinct keys, so sorting can be done faster. 

If we are willing to spend $\Theta(n + \log (np))$
extra space, counting sort can sort the tasks (based on the
rounded keys) in time $\Theta(n+\log(np))$. 
If we want to still only use $O(1)$ space, then by
sequentially partitioning on each bit of the key's ranks we get
an in-place algorithm with running time $O(n \log \log (np))$.

Since the tasks are only approximately sorted we cannot
guarantee competitive ratio $2$ with this method. However, we can
guarantee that we are $8$-competitive: with the exception of the
tasks with work smaller than $1/(np)$ we have increased each
task's size by at most a factor of $2$; increasing the serial
works of all tasks by a factor of $2$ obviously doubles the awake
time that we achieve. Now consider the tasks with works at most
$1/(np)$. The sum of their works is at most $1/p$, and the
largest task has work $1$, and hence takes time at least $1/p$ to
run.

Another interesting assumption to remove is the assumption that
we know the works of the parallel implementations of tasks: it
turns out that, if we are willing to sacrifice a constant-factor
in the competitive ratio, we do not even need to know the
parallel works of the tasks before running them! Now we give an
algorithm that is $O(1)$-competitive with OPT, which \emph{does not
use the size of the parallel implementations of tasks}: it uses
the size of the serial tasks to decide which tasks to run, and
then is told how long parallel tasks took to run after running
them. We present this algorithm in Algorithm~\ref{alg:run}, which
we call \defn{$\run$} We remark that the same techniques discussed
above for removing the assumption that tasks are pre-sorted in
THRESH could be used in $\run$.

\begin{algorithm}
  \caption{$\run$}
  \label{alg:run}
  \begin{algorithmic}
    \State \textbf{Input: } a list of tasks $\tau_1, \tau_2,
    \ldots, \tau_n$, sorted in descending order by
    $\sigma(\tau_i).$
    \State \textbf{Output: } a schedule that is $O(1)$-competitive
    with the schedule FROST would make
    \State 
    \State $w_\pi \gets 0$
    \State $i \gets 1$
    \While{$i \le n$}
      \If{$w_\pi > \sigma(\tau_i)$}
        \State \textbf{break}
      \EndIf
      \State Run task $\tau_i$ in parallel
      \State $w_\pi \gets w_\pi + \pi(\tau_i)/p$
      \State $i \gets i+1$
    \EndWhile
    \While{$i \le n$}
      \State Schedule $\tau_i$ in serial on $\rho_{1+(i \bmod p)}$
      \State $i \gets i+1$
    \EndWhile
  \end{algorithmic}
\end{algorithm}

Note that $\run$ accesses $\pi(\tau_i)$ only after running $\tau_i$
in parallel, after which it could of course have measured
$\pi(\tau_i)$. $\run$ and THRESH give very similar strategies.
$\run$'s strategy can be slightly worse than THRESH's though, so $\run$
has a slightly worse competitive ratio than THRESH versus OPT.

\begin{proposition}
  $\run$ is $4$-competitive with OPT.
\end{proposition}
\begin{proof}
  Let $\tau_{i_*}$ be the task with the largest serial work that
  $\run$ schedules in serial.
  Since $$\sum_{i < i_*} \pi(\tau_i) / p > \sigma(\tau_{i_*})$$
  it is clearly impossible for OPT to achieve awake time better
  than $\sigma(\tau_{i_*})$: if OPT schedules everything with
  serial work at least $\sigma(\tau_{i_*})$ in parallel OPT's
  awake time is at least $\sigma(\tau_{i_*})$, and if OPT
  schedules any tasks with serial work at least
  $\sigma(\tau_{i_*})$ in serial OPT's awake time is at least
  $\sigma(\tau_{i_*})$.

  The running time of OPT is also of course at least 
  $$\sum_{i=1}^n \sigma(\tau_i)/p.$$

  The running time of the parallel tasks for $\run$ is clearly at
  most $2\sigma(\tau_{i_*})$.

  The running time for the parallel tasks of $\run$ is, by the same
  analysis presented in the analysis of THRESH, at most 
  $$\sum_{i > i_*}\sigma(\tau_i) / p + \sigma(\tau_{i_*}).$$

  Combined we have the upper bound of 
  $$\sum_{i > i_*}\sigma(\tau_i) / p + 3\sigma(\tau_{i_*})$$ 
  on the awake time of $\run$. Using our lower bounds on the awake
  time of OPT we have that $\run$ is $4$-competitive.

\end{proof}

By modifying $\run$ we can get a better competitive ratio.
The new version of $\run$ will have a parameter $\eta \in (0,1)$.
The new version of $\run$ proceeds as follows:
\begin{itemize}
  \item Run tasks in parallel (in order from tasks with largest
    serial work to smallest) until the amount of parallel work
    exceeds $p \eta \sigma(\tau_{i_*})$ where $\tau_{i_*}$ is the
    task currently running.
  \item Using bin-packing schedule in serial the tasks
    $\tau_{i_*+1}, \tau_{i_*+2}, \ldots, \tau_n$. Keep running
    $\tau_{i_*}$, but redistribute its work so that it falls in
    the gaps in the bin-packing, if possible, and so that it
    falls evenly on top of the bins if placing it solely in the
    gaps in impossible.
\end{itemize}

\begin{proposition}
  \label{prop:runetaupper}
  $\run(\eta)$ is $\max(1+1/\eta, 2+\eta)$-competitive.
\end{proposition}
\begin{proof}
  \todo{ this proof is pretty sketchy, but has some interesting
  ideas.}
    Let $w_\pi^f$ be the final total parallel work performed by
    $\run(\eta)$, divided by $p$. Let $\tau_{i_*}$ be the task
    with the largest serial work that $\run(\eta)$ runs in
    parallel.

    We consider $2$ cases.

  \noindent\textbf{Case 1:} $w_\pi^f \approx \eta \sigma(\tau_{i_*})$\\
    OPT may choose to run all tasks $\tau_i$ with $i \le i_*$ in
    parallel, like $\run(\eta)$ does. In this case we have
    $$T^{OPT} \ge \eta \sigma(\tau_{i_*}) + \sum_{i>i_*}
    \sigma(\tau_i)/p.$$
    On the other hand 
    $$T^{\run(\eta)} \le \eta \sigma(\tau_{i_*}) + \sum_{i>i_*}
    \sigma(\tau_i)/p + \sigma(\tau_{i_*+1}) \le
    (1+1/\eta)T^{OPT}.$$
    OPT might also choose to run some task $\tau_i$ with $i
    \le i_* $ in serial. This would make
    $$T^{OPT} \ge \sigma(\tau_{i_*}).$$
    On the other hand 
    $$T^{RUN} \le \eta \sigma(\tau_{i_*}) + T^{OPT} \le
    (1+1/\eta)T^{OPT},$$
    \todo{because OPT can't beat our bin packing since its on
    these small tasks.}

    \noindent\textbf{Case 2:} $w_\pi^f \approx (\eta+1) \sigma(\tau_{i_*})$
    Here we have $T^{OPT} \ge \sigma(\tau_{i_*}), T^{OPT} \ge
    \sum_{i=1}^n \sigma(\tau_i)/p$.
    In this case the task can probably fit in the cracks of our
    bin-packing, if we assume somewhat good bin-packing this
    would mean 
    $$T^{\run(\eta)} \le (1+\eta)\sigma(\tau_{i_*}) +
    \sum_{i=1}^n \sigma(\tau_i)/p.$$
    So this would be $(2+\eta)$-competitive with OPT.
\end{proof}
\begin{proposition}
  \label{prop:runetalower}
  $\run(\eta)$ is not $(1 + (1-2/p)/\eta)$-competitive, and $\run$ is
  not $(2+\eta)(1-3/p)$-competitive.
\end{proposition}
\begin{proof}
  Consider the following TAP: at time $0$ two perfectly scalable
  tasks arrive: one with serial work $\eta p$, the other with
  serial work $1$. Clearly OPT takes time $\eta + 1/p$ to do both
  tasks. On the other hand $\run(\eta)$ starts running the larger
  task in parallel, and by the end of it has done parallel work
  $\eta$. But this is at least $1\cdot \eta$, so $\run(\eta)$
  runs the other task in serial, hence achieveing awake time
  $1+\eta$. The cost ratio here is 
  $$\frac{1+\eta}{1/p+\eta} \ge (1-1/(\eta p))(1+1/\eta) \ge
  1+ \frac{1}{\eta} (1-2/p).$$

 Now consider the following TAP: a task with serial work
 $1+1/p$ and parallel work $p(1+1/p)$ arrives along with
 $p+1$ perfectly scalable tasks with serial work $1$. Clearly OPT
 has awake time $1+1/p + 2/p$ by running the biggest task in
 serial along with $p-1$ tasks with serial work $1$, and then
 after they all finish running the final $2$ tasks in parallel
 across all the processors.
 On the other hand, $\run(\eta)$ runs the biggest task in
 parallel, and then runs the $p+1$ smaller tasks in serial so it
 takes time $\eta + 1 + 1$. Hence $\run(\eta)$'s competitive
 ratio on this TAP is $$\frac{\eta+2}{1+1/p+2/p}\ge
 (\eta+2)(1-3/p).$$
\end{proof}

Combined, Proposition~\ref{prop:runetaupper} and
Proposition~\ref{prop:runetalower} basically imply:
\begin{corollary}
  For $\eta=\frac{\sqrt{5}-1}{2}$, $\run(\eta)$ is
  $\frac{3+\sqrt{5}}{2}$-competitive with OPT.
  For all $\eta$, $\run(\eta)$ is not
  $\left(\frac{3+\sqrt{5}}{2}-1/p\right)$-competitive.
\end{corollary}
\begin{proof}
  It is clear that the competitive-ratio is minimized when
  $$1+1/\eta = 2+\eta.$$
  Which happens at $\eta=\frac{\sqrt{5}-1}{2}$.
\end{proof}

While $\run(\eta)$ can do remarkably well for the case where the
parallel works are not known, it is not clear that $\run(\eta)$
is optimal.
\begin{proposition}
  Any on-line scheduling algorithm for the variant of the game
  where the parallel works are not known is worse than 
  $2(1-1/p)$-competitive on some TAP.
\end{proposition}
\begin{proof}
  Consider a TAP with $p+1$ tasks all having identical serial
  work $1$, with $p$ tasks that have parallel work $p$ and a
  single task that has parallel work $1$. 
  OPT clearly achieves awake time $1+1/p$, whereas no on-line
  algorithm can get awake time better than $2$ in general. 
  Hence the competitive ratio is 
  $$\frac{2}{1+1/p} \le 2(1-1/p).$$
\end{proof}

\subsection{Arbitrary speedup curves} 
So far we have only considered the \defn{perfect} speedup curve,
i.e. $x\mapsto x$, where splitting a parallel task amongst $x$
processors results in each processor getting work $\pi(\tau)/x$.
Note that although we have been assuming that the parallel
implementations scale perfectly, we have not been assuming that
the parallel implementations are work-efficient; note that if all
parallel implementations were work-efficient and scaled perfectly
then obviously every task should be scheduled in parallel across
all the processors. In this Subsection we consider a
generalization of the problem to non-perfect speedup curves.
However, we do still impose some reasonable restrictions on the
speedup curves. The speedup curves we consider will be
(non-strictly) sublinear (i.e. $f(k) \le k$),
monotonically increasing functions $f: [p] \to [1,p]$, with $f(1)
= 1$. In the generalized problem, we denote by $f_\tau$ the
speedup curve associated with $\tau$, and still denote with
$\sigma(\tau)$ and $\pi(\tau)$ the works of the serial and
parallel implementations of $\tau$. Now however, if the task is
run equally split in parallel across $k$ processors, the time it
takes is $\pi(\tau) / f_\tau(k)$ per processor.

It is clear that Theorem~\ref{thm:constant_competitive} still
applies: if we have a scheduling algorithm $\oracle_R$ that is
$R$-competitive with OPT in the off-line problem, then $\bat_R$
is $2R$-competitive with OPT in the on-line problem. The task
thus becomes to find algorithms for $\oracle_R$.

There is still a straightforward algorithm for $\oracle_1$ in
this case:
\begin{enumerate}
  \item Perform a brute force search over every
possible subset of the tasks to be the set of tasks scheduled in
parallel, and do a brute force search over every way to assign
the serial tasks to processors. 

\item While the amount of work assigned to all the tasks is not
  even, we continue to assign parallel work as follows. Let $k$
  be the current number of processors with the minimum amount of
  work, let $\alpha, \beta$ be the minimum, and second lowest
  amounts of works assigned to any processor. Find whichever task
  scales the best on $k$ processors, and schedule $k(\beta -
  \alpha)$ parallel work from this task to these $k$ processors,
  or all the work that the task has if the task has less than
  $k(\beta-\alpha)$ work. If the amount of work assigned to all
  the tasks becomes even, then simply schedule all the remaining
  work from the remaining parallel tasks evenly in parallel
  across all the processors.
\end{enumerate}
Step 2 of this process is slightly more complex than previously,
but the running time remains the same (asymptotically) as
$(\Theta(p))^n$.

Now we consider how to make efficient algorithms for the
non-perfect speedup curves case. We prove a striking lower bound,
that indicates that fundamentally different strategies are needed
in this case.
\begin{proposition}
  Any (off-line) scheduler that does not ever schedule a task on
  an intermediaite number of processors, i.e. a scheduler that
  always either assigns a task to a single processor or runs the
  task in parallel across all the processors, cannot achieve a
  competitive ratio better than $\sqrt{p}$ versus OPT.
\end{proposition}
\begin{proof}
  Consider a TAP where $\sqrt{p}$ identical tasks arrive at the
  beginning, with the following characteristics: $\sigma(\tau) =
  1$, $\pi(\tau) = 1$, and 
  $$
  f_\tau(k) = 
  \begin{cases}
    k & k \in [1,\sqrt{p}]\\
    \sqrt{p} & k \in [\sqrt{p}, p].
  \end{cases}$$
  OPT clearly schedules all tasks in parallel on $\sqrt{p}$
  processors, and hence achieves awake time $1/\sqrt{p}$.

  A scheduler that schedules all tasks in serial achieves awake
  time $1$. A scheduler that schedules all tasks in parallel
  across all the processors achieves awake time
  $\sqrt{p}/\sqrt{p} = 1.$
  Either way, a scheduling algorithm that does not consider
  scheduling tasks on intermediate numbers of processors has
  competitive ratio $\sqrt{p}$ on this input.

\end{proof}


\subsection{Lower Bound against Deterministic Algorithms}
In this Subsection we prove several impossibility results, which
show that we cannot hope to substantially improve our algorithms.

We can specify a TAP in a table with a list of which tasks arrive
at each time; we use the compact notation $(\sigma, \pi)\times m$
to denote $m$ identical tasks with serial works $\sigma$ and
parallel works $\pi$. It is clear that $\bat_1$ is not
$(2-\epsilon)$-competitive for any $\epsilon > 0$; consider, for
example the TAP given in Table~\ref{tab:2minusEps}, on which BAT
achieves awake time $2$ and OPT achieves awake time
$1+\varepsilon$.

\begin{table}[H]
\caption{}
\label{tab:2minusEps}
\centering
\begin{tabular}{|l|l|}
\hline
time & tasks                    \\ \hline
$0$  & $(1,p) \times p/2$       \\ \hline
$\varepsilon$  & $(1, p) \times p/2$ \\ \hline
\end{tabular}
\end{table}

We might hope to achieve a $(1+\varepsilon)$-competitive scheduling
algorithm for the on-line scheduling problem, which would be
better than $\bat_1$ in terms of competitive ratio. However, we
show that it is impossible for an off-line deterministic
scheduler to get a competitive ratio lower than $1.25$, even
using preemption. That is, we show that for any deterministic
algorithm ALG there is some input on which ALG has awake time at
least $1.25$ times greater than OPT. 

In Table~\ref{tab:lowerboundFork1} and
Table~\ref{tab:lowerboundFork2} we specify two TAPs.

\begin{table}[H]
\caption{}
\label{tab:lowerboundFork1}
\centering
\begin{tabular}{|l|l|}
\hline
time & tasks                    \\ \hline
$0$  & $(4, 2p) \times 1$       \\ \hline
$1$  & $(3, 3p) \times (p-1)$ \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\caption{}
\label{tab:lowerboundFork2}
\centering
\begin{tabular}{|l|l|}
\hline
time & tasks                    \\ \hline
$0$  & $(4, 2p) \times 1$       \\ \hline
\end{tabular}
\end{table}

Consider an arbitrary deterministic scheduling algorithm. If at
time $0$ the arriving tasks are $(4, 2p)\times 1$ (i.e. a single
task arrives, with serial work $4$ and parallel work $2p$) then
the scheduler has two options: it can schedule this task in
serial, or in parallel.

If no further tasks arrive, i.e. the task schedule is from
Table~\ref{tab:lowerboundFork2} then OPT would have awake time
$2$ by distributing the tasks work equally amongst all
processors, whereas a scheduler that ran the task in serial for
all of the time that it was running the task during the first
second after the task arrived would have awake time at least $3$.
In this case the competitive ratio of the algorithm is at least $1.5$.

On the other hand, the algorithm could decide to not run the task
in serial for any time during the first second after the task
arrives. In this case, if
and it turns out that the task schedule is from
Table~\ref{tab:lowerboundFork1}, then the algorithm has again
acted sub-optimally. In particular, for the schedule given in
Table~\ref{tab:lowerboundFork1}, OPT schedules the task that
arrives at time $0$ in serial, and then schedules all the tasks
that arrive at time $1$ in serial as well, and hence achieves
awake time $4$. On the other hand, the awake time of an algorithm
that did not schedule the task that arrived at time $0$ in
serial is at least $5$: such a scheduler may either choose at
time $1$ to cancel the task from time $0$ and run it in serial,
or the scheduler may choose to let the parallel implementation
finish running. In this case the competitive ratio of the
algorithm is $5/4$.

Hence it is impossible for any deterministic algorithm to achieve
a competitive ratio of lower than $1.25$.

We remark that the numbers in this argument can clearly be
optimized; doing so gives an improved lower bound of about $1.36$
on competitive ratio. 

\subsection{Lower Bound against Randomized Algorithms}
Often randomized algorithms can achieve improvements over
deterministic algorithms, so we for example hope for a
$(1+\epsilon)$-competitive randomized on-line scheduling
algorithm that succeeds on any input with high probability.
However, in this subsection we show that this is impossible.

In particular, we demonstrate a lower-bound of $1.0625$ on the
competitive ratio of any randomized off-line algorithm.

Recall the TAPs from Table~\ref{tab:lowerboundFork1} and
Table~\ref{tab:lowerboundFork2}; we will use these as sub-parts
of our the TAP that we build to be adversarial for a randomized
algorithm. 

Fix some off-line randomized algorithm RAND. We say that an input
TAP is \defn{bad} for RAND if with high probability RAND's awake
time on TAP is at least $1.0625$ times that of OPT.
We construct a class of TAPs, and show that some of the TAPs in
this class must be bad for RAND.

Let $\mathcal{T}_{I}$, for some some binary string $I$, be the
TAP consisting of the TAP from Table~\ref{tab:lowerboundFork1} at
time $10i$ if $I_i = 1$ and the TAP from
Table~\ref{tab:lowerboundFork2} if $I_i = 0$. 

Consider a $I$ chosen uniformly at random from $\{0,1\}^m$ for
some parameter $m$.
On each sub-tap RAND has at most a $1/2$ chance of acting as OPT
does, and at least a $1/2$ chance of acting sub-optimally, in
particular, from our analysis above showing that any deterministic
algorithm has competitive ratio at least $1.25$ on at least one
of these inputs, RAND has at least a $1/2$ chance of this
happening.
By a Chernoff Bound, with probability at least
$1-e^{-\Omega(m)}$, on at least $1/4$ of the sub-taps RAND has
competitive ratio at least $1.25$. Since there is no overlap, by
design, of the sub-taps (by spacing them out), this means that
overall the competitive ratio of RAND is at least $1\cdot 3/4 +
1.25 \cdot 1/4 = 1.0625.$

Note that the number of tasks in such a TAP is less than $mp$, so
$n \le mp$, and thus $m \ge n/p$.
Hence our result that holds with high probability in $m$ holds
with high probability in $n/p$ too.
Of course $n\gg p$ so this is pretty decent.

Because a randomly chosen TAP from this class of TAPs is bad for
RAND with high probability in $n/p$, by the probabilistic method
there is at least one TAP in this class of TAPs that is bad for
RAND. 



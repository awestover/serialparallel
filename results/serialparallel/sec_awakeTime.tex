\section{Minimizing Awake Time on TAPs} 
\label{sec:awaketime}
In this section we give a simple deterministic scheduling
algorithm --- that does not use preemption --- for minimizing
awake time. We show that our algorithm is $2$-competitive
with OPT on all TAPs. Our algorithm is theoretically interesting,
but not efficient. We also give an efficient algorithm for
$2$-approximating our algorithm.

Note that without loss of generality we may consider TAPs where
the cost ratio $\pi(\tau)/\sigma(\tau) \in [1,p]$ for all tasks
$\tau$; if $\pi(\tau)/\sigma(\tau) < 1$ then the scheduler
clearly should never run $\tau$ in serial so we can replace the
serial implementation with the parallel implementation to get
cost ratio $1$, similarly, if $\pi(\tau)/\sigma(\tau) > p$ then
the scheduler should never run $\tau$ in parallel and we can
replace the parallel implementation with the serial
implementation to get cost ratio $p$.

We say that a time is a \defn{verge} time for our algorithm if at
this time no processors have work assigned to them, and there is at
least one ready task.

We propose Algorithm~\ref{alg:bat}, which we call \defn{BAT}
(BAT is short for \enquote{batch}), as a scheduling algorithm.

\begin{algorithm}
  \caption{BAT}
  \label{alg:bat}
  \begin{algorithmic}
    \State Let $\oracle_R$ be an algorithm that is $R$-competitive with FROST 
    \If{verge time}
      \State schedule tasks as directed by $\oracle_R$
    \EndIf
  \end{algorithmic}
\end{algorithm}

A \defn{single-time-TAP} is a TAP where all tasks arrive at a
single time. FROST is an algorithm that yields a schedule
achieving minimal awake time, i.e. the same awake time as OPT, on
single-time-TAPs. In Lemma~\ref{lem:frosting} we establish that
Algorithm~\ref{alg:frost} implements FROST, hence showing
that BAT can actually be computed. Algorithm~\ref{alg:frost}
exactly computes FROST, but is very slow. We also give an
algorithm THRESH in Algorithm~\ref{alg:thresh} for
$2$-approximating FROST in linear time. We remark that it is not
obvious that an oracle for OPT can be computed in finite time,
even in this special case: there are an uncountably infinite
number of possible scheduling strategies that OPT can choose
from. Nevertheless, we show how to consider a finite search space
that a search can actually be performed on, at least in the
special case of single-time-TAPs. Before considering FROST, and
approximations to FROST, we analyze the competitive ratio of BAT
assuming that we have $\oracle_R$, an algorithm that is
$R$-competitive with FROST for some $R \le O(1)$; $\oracle_1$
corresponds to FROST while $\oracle_2$ corresponds to THRESH.

Consider a TAP $\mathcal{T}$. Let $\ell$ be the number of verge
times for $\mathcal{T}$; note that $\ell \le n$ which in
particular is finite. Let $t_i$ be the $i$-th verge time, and
define $t_0$ (not a verge time) to be $-\infty$ for convenience.
Let $T^{ALG}(a, b)$ denote the awake time of a scheduling algorithm
ALG on the truncation of the TAP $\mathcal{T}$ that only consists
of tasks arriving at times in $(a, b]$.

Note that all tasks that arrive at times in $(t_0, t_1]$ arrive
at time $t_1$. By definition of $\oracle_R$ we have that
$\oracle_R$ is $R$-competitive with OPT on single-time-TAPs, i.e.
\begin{equation}
  \label{eq:same_single}
  T^{BAT}(t_0,t_1) \le R \cdot T^{OPT}(t_0,t_1).
\end{equation}

An \defn{ALG-gap} is an interval of time $I$ of non-zero length where for
all times in the interior of $I$ ALG has completed every
task that has arrived thus far. Additionally, for an interval to
be an ALG-gap the interval must contain no other intervals which
are also ALG-gaps (i.e. it is a \enquote{maximal} interval
satisfying our conditions).
We say that a TAP is \defn{ALG-gap-free} if it contains no ALG-gaps.

Now we prove an obvious property of OPT.
\begin{claim}
  \label{clm:OPT_finishes_first}
  If there is a scheduling algorithm ALG that completes all tasks by
  time $t_*$ then OPT finishes all tasks by time $t_*$.
\end{claim}
\begin{proof}
  Say that ALG completes all tasks by time $t_*$. Let $t_0 < t_*$
  be the most recent time that OPT has completed all tasks that
  arrive before time $t_0$. If OPT has not finished all tasks by
  time $t_*$ then it was acting sub-optimally, as it could steal
  the strategy that ALG used on $[t_0, t_*]$ to achieve lower
  awake time. In particular, for any tasks that arrive in $[t_0,
  t_*]$ OPT could schedule them as ALG schedules them. 
\end{proof}
As an immediate consequence of Claim~\ref{clm:OPT_finishes_first}
we have that any ALG-gap is a subset of an OPT-gap.

Decomposing TAPs into gap-free subsets of the TAP is very useful.
Part of the reason for this is the following fact:
\begin{claim}
  \label{clm:just_consider_gapless}
  If an algorithm ALG achieves competitive ratio $r$ on
  ALG-gap-free TAPs, then ALG achieves 
  competitive ratio $r$ on arbitrary TAPs.
\end{claim}
\begin{proof}
  We define an equivalence relation $\tau_i \sim \tau_j$ to mean
  that no ALG-gap separates $\tau_i,\tau_j$. $\sim$ partitions
  the tasks into sets $I_1, \ldots, I_m$. Let
  $s(I_k)=\min_{\tau\in I_k}(t(\tau)), f(I_k)= \max_{\tau\in
  I_k}(t(\tau)).$ We clearly have 
  $$T^{ALG}(t_1, t_n) = \sum_{k=1}^m T^{ALG}(s(I_k), f(I_k)).$$
  By Claim~\ref{clm:OPT_finishes_first}, ALG-gaps are subsets of OPT-gaps.
  Hence, we also have 
  $$T^{OPT}(t_1, t_n) = \sum_{k=1}^m T^{OPT}(s(I_k), f(I_k)).$$
  On the truncation of the TAP to a gap-free set of tasks ALG is
  $r$-competitive with OPT by assumption, so we have
  $$T_{I_k}^{ALG}(s(I_k), f(I_k)) \le r\cdot
  T_{I_k}^{OPT}(s(I_k), f(I_k)).$$
  Summing this, we get $$T^{ALG}(t_1, t_n) \le r\cdot T^{OPT}(t_1,
  t_n),$$ as desired.
\end{proof}

By Claim~\ref{clm:just_consider_gapless}, in order to bound BAT's
competitive ratio, it suffices to consider TAPs
without BAT-gaps. Note however that a TAP without
BAT-gaps could still have OPT-gaps.

We conclude our analysis of the competitive ratio of BAT in
Theorem~\ref{thm:constant_competitive} with an inductive argument on
the number of OPT-gaps in the TAP.
First we establish the base case for the argument in
Proposition~\ref{prop:no_optgaps}: we consider
BAT's competitive ratio on an OPT-gap-free TAP.  

\begin{proposition}
  \label{prop:no_optgaps}
  BAT is $(R+1)$-competitive on OPT-gap-free TAPs.
\end{proposition}
\begin{proof}
  For an OPT-gap-free TAP (which is of course also a BAT-gap-free
  TAP) we must have
  \begin{equation}
    \label{eq:opt_isnt_so_much_better}
    T^{OPT}(t_1, t_n) \ge t_{n-1} - t_1 = T^{BAT}(t_1, t_{n-1}),
  \end{equation}
  because some tasks arrive at time $t_1$, and some tasks arrive
  after $t_{n-1}$, so for both BAT and OPT to have no gaps their
  awake times must satisfy the specified constraints.

  Because BAT finishes all $q_{i}$ tasks that arrive at time $t_i$
  by time $t_{i+1}$ we can actually always decompose
  $T^{BAT}(t_1, t_n)$ as 
  \begin{equation}
    \label{eq:decomposeBAT}
    T^{BAT}(t_1, t_n) = \sum_{i=1}^n T^{BAT}(t_{i-1}, t_i).
  \end{equation}
  By Equation~\eqref{eq:decomposeBAT}, and
  Equation~\eqref{eq:same_single} we thus have 
  \begin{equation}
    \label{eq:decompose_rearanged}
    T^{BAT}(t_1, t_n) \le T^{BAT}(t_1, t_{n-1}) + R \cdot T^{OPT}(t_{n-1}, t_n).
  \end{equation}

  Hence by Equation~\eqref{eq:opt_isnt_so_much_better} and
  Equation~\eqref{eq:decompose_rearanged} we have
  \begin{align*}
    T^{BAT}(t_1, t_n) &\le T^{OPT}(t_1, t_n) + R\cdot T^{OPT}(t_{n-1}, t_n)\\
                                 &\le (R+1)T^{OPT}(q_1, \ldots, q_\ell),
  \end{align*}
  as desired.
\end{proof}

\begin{theorem}
  \label{thm:constant_competitive}
  BAT is $(R+1)$-competitive.
\end{theorem}

{\color{red}
  Bill's proof of this:

  Let an OPT-batch be a maximal set of tasks done by OPT. Let a
  BAT-batch be one of BAT's batches. Call a BAT-batch
  \defn{internal} if it is a subset of an OPT-batch. Call a
  BAT-batch \defn{external} if it is not a subset of an
  OPT-batch. 

  Consider an OPT-batch $x$. Let $B_1$ be the union of the
  internal BAT-batches contained in $x$, let $B_2$ be the
  BAT-batch that overlaps with the end of $x$ (if such a batch
  exists), and let $B_3$ be the first BAT-batch that starts after
  $x$. Let $T_1$ be the time since the start of $x$ to the start
  of $B_2$ and let $T_2$ be the time from the start of $B_2$
  until the end of $x$. The amount of time that OPT spends on
  tasks from $x$ is clearly $T_1+T_2$. Now we bound how much time
  BAT spends on tasks from $x$. $B_1$ takes time $T_1$. $B_3$
  spends time at most $RT_2$ on tasks from $x$. $B_2$ spends time
  at most $R(T_1 + T_2)$ on tasks from $x$. Adding these times up
  we get that BAT spends at most $2R(T_1+T_2)$ time on the tasks
  from $x$. Adding this up across all OPT-batches $x$, gives that
  BAT is $2R$-competitive, as desired.

}

\begin{proof}
  By Claim~\ref{clm:just_consider_gapless} without loss of
  generality we consider TAPs without ALG-gaps.

  The proof is by strong induction on the number of OPT-gaps. 
  The base case of our induction is established in
  Proposition~\ref{prop:no_optgaps}, which says that if there are $0$
  OPT-gaps then BAT is $2$-competitive. 

  Consider a TAP that has more than $k > 0$ OPT gaps, assume that
  for any TAP with fewer than $k$ OPT gaps BAT is $2$-competitive
  with OPT.

  Let the first OPT-gap of the TAP start at time $t_*$.
  Let $j$ be an index such that $t_j < t_* < t_{j+1}$.

  Using our inductive hypothesis we have:
  \begin{align*}
  &T^{OPT}(t_1, t_n) \\
  &= T^{OPT}(t_1, t_*) + T^{OPT}(t_*, t_n)\\
  &\ge \frac{1}{R+1}\paren{T^{BAT}(t_1, t_*) + T^{BAT}(t_*, t_n)}\\
  &=\frac{1}{R+1} T^{BAT}(q_1, \ldots, q_\ell).
  \end{align*}

  Rearranging we get the desired bound:
  $$T^{BAT}(q_1, \ldots, q_\ell) \le (R+1) T^{OPT}(q_1, \ldots, q_\ell).$$
\end{proof}

Now we analyze Algorithm~\ref{alg:frost}, which we call
FROST, or $\oracle_1$. 

\begin{algorithm}
  \caption{FROST (i.e. $\oracle_1$)}
  \label{alg:frost}
  \begin{algorithmic}
    \State \textbf{Input:} tasks $\tau_1,\ldots, \tau_n$ all with $t(\tau_i) = 0$
    \State \textbf{Output:} a way to schedule the tasks to
    processors $\rho_1, \ldots, \rho_p$ that achieves minimal awake time
    \State 
    \State $\text{minAwakeTime} \gets \infty$
    \State $\text{bestSchedule} \gets $ schedule everything in serial on $\rho_1$
    \For{$I \in \{0,1\}^n$} 
      \State $x \gets \sum_{i=1}^n I_i$
      \For{$J \in \{1, \ldots, p\}^x$}
        \State $j \gets 0$
        \For{$i \in \{1,2,\ldots, n\}$}
          \If{$I_i=1$}
            \State $j \gets j+1$
            \State schedule task $\tau_i$ in serial on $\rho_{J_j}$
          \EndIf
        \EndFor
        \State $m \gets \max_{\rho_i}(\work(\rho_i))$
        \State $w \gets \sum_{\rho_i} \paren{m - \work(\rho_i)}$
        \State $f \gets \sum_{\rho_i} (1-I_i)\pi(\rho_i)$
        \If{$f \ge w$}
          \State make $\rho_i$ have work $m + (f-w)/p$
        \Else 
          \State distribute $f$ units of work arbitrarily 
          \State among $\rho_i$ without increasing awake time
        \EndIf
        \If{$\text{awakeTime}(I, J) \le \text{minAwakeTime}$}
          \State{$\text{minAwakeTime} \gets \text{awakeTime}(I, J)$}
          \State{$\text{bestSchedule} \gets \text{schedule}(I, J)$}
        \EndIf
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

The key insight to decrease the search space to be finite is to
notice that for any method of distributing whichever tasks are
chosen to run in serial, the parallel tasks may as well be
redistributed afterwords, so long as doing so either results in
not increasing the awake time, or results in all tasks having
identical amounts of work.
We can thus do a brute-force search over all the ways to assign some
tasks to run in serial and to run on specific processors, and
then put the parallel tasks on top \enquote{like frosting on a cake}. 

We remark that the running time of FROST for $n$ tasks is 
larger than $\Omega(p^n)$, which is extremely large. Nevertheless the
existence of the algorithm is interesting. Later we give an
algorithm for $\oracle_2$ that has much more reasonable running
time.

We now prove that FROST actually does compute a schedule with
awake time the same as that of OPT.
\begin{lemma}[Frosting Lemma]
  \label{lem:frosting} 
  FROST is an oracle for OPT on TAPs where all tasks arrive at a single time.
\end{lemma}
\begin{proof}
  Consider the configuration that OPT chooses. FROST considers
  a configuration of tasks with the same assignment of serial
  tasks at some point, because FROST brute force searches
  through all of these. For this configuration it is clearly
  impossible to achieve lower awake time than by spreading the
  parallel tasks in the frosting method, hence OPT's awake time
  is at least that of FROST.
\end{proof}

We remark that it is straightforward to extend FROST to a full
oracle for OPT. In particular, a full OPT oracle can be
constructed as follows. First, perform a brute force search over every
possible subset of the tasks to be the set of tasks scheduled in
parallel, and do a brute force search over every way to assign
the serial tasks to processors. Now consider the gaps, i.e. times
when all processors are idle under this current schedule. Add the
parallel tasks, scheduling them so as not to decrease the size of
any gaps unless it is impossible to do otherwise, in which case
the parallel tasks should be added to make all processors have
equal amounts of work extending into what used to be a gap.

We now consider how to more efficiently compute the best schedule
for the case where all tasks arrive at the start. In fact, we
consider making an algorithm to get a constant approximation of
this, which will still give an algorithm with constant
competitive ratio.

We propose Algorithm~\ref{alg:thresh}, which we call THRESH, as a
simple way to $2$-approximate FROST.

\begin{algorithm}
  \caption{THRESH}
  \label{alg:thresh}
  \begin{algorithmic}
    \State \textbf{Input:} tasks $\tau_1,\ldots, \tau_n$ with
    $\sigma(\tau_1) \ge \sigma(\tau_2)\ge \cdots \ge
    \sigma(\tau_n)$ all with $t(\tau_i) = 0$
    \State \textbf{Output:} a way to schedule the tasks to
    processors $\rho_1, \ldots, \rho_p$ that achieves awake time
    at most twice the optimal awake time.
    \State
    \State $w_\sigma \gets \sum_i \sigma(\tau_i)$
    \State $w_\pi \gets 0$
    \State $a \gets 0$, $i_* = 0$
    \For{$i \in [n]$}
      \State $w_\sigma \gets w_\sigma - \sigma(\tau_i)$
      \State $w_\pi \gets w_\pi + \pi(\tau_i)$
      \If{$i < n$}
        \State $a_0 = w_\sigma / p + \sigma(\tau_{i+1}) + w_\pi/p$
      \Else
        \State $a_0 = w_\pi/p$
      \EndIf
      \If{$a_0 \le a$}
        \State $i_* \gets i$
        \State $a \gets a_0$
      \EndIf
    \EndFor
    \State Schedule tasks $\tau_1, \ldots, \tau_{i_*}$ in
    parallel, distributing their work equally among the
    processors.
    \For{$k \in \{i_* + 1, \ldots, n\} $}
    \State Schedule $\tau_{k}$ in serial on processor $\rho_{1+(k \bmod p)}$
    \EndFor
  \end{algorithmic}
\end{algorithm}

Put simply THRESH schedules the $i_*$ tasks with largest serial
work in parallel, distributing their work equally, and schedules
the rest of the tasks in serial, sequentially giving out the
tasks, for the optimal value of $i_*$.

Clearly THRESH requires space $\Theta(1)$ beyond the input to
implement, and has running time $\Theta(n)$, given that the input
is pre-sorted.

In order to remove the assumption that the input is pre-sorted,
we can sort the tasks at the start of the algorithm, using heap
sort or radix sort. If we simply use heap sort then we clearly
will have running time $\Theta(n \log n)$ but still have $O(1)$
space requirement, and have competitive ratio $2$ with FROST.
On the other hand, if we use radix sort, it is conceivable that
we could do better. First, if the tasks can be assumed to have
serial works that can only take on $2^{O(1)}$ possible values,
then the radix sort can be performed in linear time, in-place,
resulting in a $2$-competitive algorithm that uses $O(1)$ memory
and $O(n)$ time. On the other hand, if such an assumption cannot
be made, then if better running time is still desired, then it
can be achieved at a cost to the competitive-ratio. Let $1$ be
the largest serial work of any task. If we sort the tasks based
on the key of which bucket $[1/2^i, 1/2^{i-1}]$ the task's
serial work falls in, except saying
that any tasks with work less than $1/2^{\lg (np)}$ fall in the
same bucket, then there are only $\log n + \log p$ distinct keys,
so sorting can be done much faster. If we are willing to spend
$\Theta(n + \log (np))$ space, then the sorting can be done by
counting sort in time $\Theta(n+\log(np))$. On the other hand,
using radix sort we could do the sort in time $\Theta(n \log
(np))$ using only $\Theta(1)$ auxiliary space. 
Since the tasks are only approximately sorted we can only
guarantee $4$-competitiveness in this case: this is clear by
noting that increasing the serial works of all tasks by a factor
of $2$ would double the best attainable awake time.

Now we show why THRESH is $2$-competitive.
Consider OPT's strategy.
Let $\tau_{i_0}$ be the task with the largest serial work that
OPT schedules in serial. Recalling that the tasks are sorted by
serial work, for all $i < i_0$ we have that OPT chooses to
schedule $\tau_i$ in parallel. Let $w^{OPT}_\sigma = \sum_{i \ge
i_0} \sigma(\tau_i), w^{OPT}_\pi = \sum_{i < i_0} \pi(\tau_i)$.
OPT's awake time is obviously at least $(w^{OPT}_\pi +
w^{OPT}_\sigma)/p$. OPT's awake time is also obviously at least
$\sigma(\tau_{i_0})$.

Without loss of generality we call the processor that gets the
most work, which used to be called $\rho_{1+(i_* \bmod p)}$,
$\rho_1$; in particular we circularly shift the labels of the
processors so that the work that $\rho_i$ gets is larger than the
work that $\rho_j$ gets for any $j > i$.
Think about the sequential thing.
Ignore $\sigma(\tau_{i_*})$. Then $\rho_i$ always has less work
than all the other processors. Of course in reality $\rho_1$ has
the most work of any processor.
So we have that all processors have serial work at most 
$$\sum_{i > i_*} \sigma(\tau_i)/p + \sigma(\tau_{i_*}).$$
Add on to that $$\sum_{i \le i_*} \pi(\tau_i)/p.$$

If $x \le a + b$, and $y \ge a, y \ge b$, then obviously $x \le
2y$. Thus, THRESH is $2$-competitive with FROST. 

Now we give an algorithm that is $3$-competitive with FROST,
which \emph{does not use the size of the parallel implementations
of tasks}. We present this algorithm in
Algorithm~\ref{alg:justrunit}, which we call \defn{JUSTRUNIT}.

\begin{algorithm}
  \caption{JUSTRUNIT}
  \label{alg:justrunit}
  \begin{algorithmic}
    \State \textbf{Input: } a list of tasks $\tau_1, \tau_2,
    \ldots, \tau_n$, sorted in descending order by
    $\sigma(\tau_i).$
    \State \textbf{Output: } a schedule that is $3$-competitive
    with the schedule FROST would make
    \State 
    \State $w_\pi \gets 0$
    \State $i \gets 1$
    \While{$i \le n$}
      \If{$w_\pi > \sigma(\tau_i)$}
        \State \textbf{break}
      \EndIf
      \State Run task $\tau_i$ in parallel
      \State $w_\pi \gets w_\pi + \pi(\tau_i)/p$
      \State $i \gets i+1$
    \EndWhile
    \While{$i \le n$}
      \State Schedule $\tau_i$ in serial on $\rho_{1+(i \bmod p)}$
      \State $i \gets i+1$
    \EndWhile
  \end{algorithmic}
\end{algorithm}

We remark on the very interesting property of JUSTRUNIT: it does
not need to know $\pi(\tau_i)$ until after running $\tau_i$ in
parallel, in which case it could of course have measured
$\pi(\tau_i)$. JUSTRUNIT results in a strategy very similar to
that given by THRESH. It differs slightly though, which results
in the slightly worse competitive-ratio of $3$ versus FROST. 

Next we prove several impossibility results, which show that we
cannot hope to substantially improve our algorithms.

\subsection{Lower Bound on Deterministic Algorithms for Minimizing Awake Time}
We can specify a TAP in a table with a list of which tasks arrive
at each time; we use the compact notation $(\sigma, \pi)\times m$
to denote $m$ identical tasks with serial works $\sigma$ and
parallel works $\pi$. It is clear that BAT is not
$(2-\epsilon)$-competitive for any $\epsilon > 0$; consider, for
example the TAP given in Table~\ref{tab:2minusEps}, on which BAT
achieves awake time $2$ and OPT achieves awake time
$1+\varepsilon$.

\begin{table}[H]
\caption{}
\label{tab:2minusEps}
\centering
\begin{tabular}{|l|l|}
\hline
time & tasks                    \\ \hline
$0$  & $(1,p) \times p/2$       \\ \hline
$\varepsilon$  & $(1, p) \times p/2$ \\ \hline
\end{tabular}
\end{table}

We might hope to achieve a $(1+\epsilon)$-competitive scheduling
algorithm for this problem, which would arguably be much better
than BAT. However, in this subsection we
establish that it is impossible for an off-line deterministic
scheduler to get a competitive ratio lower than $1.25$, even
using preemption. That is, we show that for any deterministic
algorithm ALG there is some input on which ALG has awake time at
least $1.25$ times greater than OPT. 

In Table~\ref{tab:lowerboundFork1} and
Table~\ref{tab:lowerboundFork2} we specify two TAPs.

\begin{table}[H]
\caption{}
\label{tab:lowerboundFork1}
\centering
\begin{tabular}{|l|l|}
\hline
time & tasks                    \\ \hline
$0$  & $(4, 2p) \times 1$       \\ \hline
$1$  & $(3, 3p) \times (p-1)$ \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\caption{}
\label{tab:lowerboundFork2}
\centering
\begin{tabular}{|l|l|}
\hline
time & tasks                    \\ \hline
$0$  & $(4, 2p) \times 1$       \\ \hline
\end{tabular}
\end{table}

Consider an arbitrary deterministic scheduling algorithm. If at
time $0$ the arriving tasks are $(4, 2p)\times 1$ (i.e. a single
task arrives, with serial work $4$ and parallel work $2p$) then
the scheduler has two options: it can schedule this task in
serial, or in parallel.

If no further tasks arrive, i.e. the task schedule is from
Table~\ref{tab:lowerboundFork2} then OPT would have awake time
$2$ by distributing the tasks work equally amongst all
processors, whereas a scheduler that ran the task in serial for
all of the time that it was running the task during the first
second after the task arrived would have awake time at least $3$.
In this case the competitive ratio of the algorithm is at least $1.5$.

On the other hand, the algorithm could decide to not run the task
in serial for any time during the first second after the task
arrives. In this case, if
and it turns out that the task schedule is from
Table~\ref{tab:lowerboundFork1}, then the algorithm has again
acted sub-optimally. In particular, for the schedule given in
Table~\ref{tab:lowerboundFork1}, OPT schedules the task that
arrives at time $0$ in serial, and then schedules all the tasks
that arrive at time $1$ in serial as well, and hence achieves
awake time $4$. On the other hand, the awake time of an algorithm
that did not schedule the task that arrived at time $0$ in
serial is at least $5$: such a scheduler may either choose at
time $1$ to cancel the task from time $0$ and run it in serial,
or the scheduler may choose to let the parallel implementation
finish running. In this case the competitive ratio of the
algorithm is $5/4$.

Hence it is impossible for any deterministic algorithm to achieve
a competitive ratio of lower than $1.25$.

We remark that the numbers in this argument can clearly be
optimized, to give an improved lower bound of about $1.36$ on
competitive ratio. As this is asymptotically not interesting, and
much messier, we decide to not give this better argument.

\subsection{Lower Bound against Randomized Algorithms}
We might that there is a randomized algorithm that gets a
competitive ratio substantially better than any deterministic
algorithm can, for example maybe there is a randomized algorithm
that is $(1+\epsilon)$-competitive on any input with high
probability, or a randomized algorithm with expected competitive
ratio at most $(1+\epsilon)$ on any input. However, in this
subsection we show that this is impossible.

In particular, we demonstrate a lower-bound of $1.0625$ on the
competitive ratio of any randomized off-line algorithm.

Recall the TAPs from Table~\ref{tab:lowerboundFork1} and
Table~\ref{tab:lowerboundFork2}; we will use these as sub-parts
of our the TAP that we build to be adversarial for a randomized
algorithm. 

Fix some off-line randomized algorithm RAND. We say that an input
TAP is \defn{bad} for RAND if with high probability RAND's awake
time on TAP is at least $1.0625$ times that of OPT.
We construct a class of TAPs, and show that some of the TAPs in
this class must be bad for RAND.

Let $\mathcal{T}_{I}$, for some some binary string $I$, be the
TAP consisting of the TAP from Table~\ref{tab:lowerboundFork1} at
time $10i$ if $I_i = 1$ and the TAP from
Table~\ref{tab:lowerboundFork2} if $I_i = 0$. 

Consider a $I$ chosen uniformly at random from $\{0,1\}^m$ for
some parameter $m$.
On each sub-tap RAND has at most a $1/2$ chance of acting as OPT
does, and at least a $1/2$ chance of acting sub-optimally, in
particular, from our analysis above showing that any deterministic
algorithm has competitive ratio at least $1.25$ on at least one
of these inputs, RAND has at least a $1/2$ chance of this
happening.
By a Chernoff Bound, with probability at least
$1-e^{-\Omega(m)}$, on at least $1/4$ of the sub-taps RAND has
competitive ratio at least $1.25$. Since there is no overlap, by
design, of the sub-taps (by spacing them out), this means that
overall the competitive ratio of RAND is at least $1\cdot 3/4 +
1.25 \cdot 1/4 = 1.0625.$

Note that the number of tasks in such a TAP is less than $mp$, so
$n \le mp$, and thus $m \ge n/p$.
Hence our result that holds with high probability in $m$ holds
with high probability in $n/p$ too.
Of course $n\gg p$ so this is pretty decent.

Because a randomly chosen TAP from this class of TAPs is bad for
RAND with high probability in $n/p$, by the probabilistic method
there is at least one TAP in this class of TAPs that is bad for
RAND. 




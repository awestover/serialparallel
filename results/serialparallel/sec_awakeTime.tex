\section{Minimizing Awake Time on TAPs} 
\label{sec:awaketime}
In this section we show that the on-line scheduling problem can
essentially be reduced to the off-line \defn{single-arrival-time}
case of the problem: the case where all tasks arrive at the
start. In particular, we give an extremely simple deterministic
scheduling algorithm, that in particular does not use preemption,
for minimizing awake time, that makes use of an algorithm for the
batch case of the off-line problem. If we use an algorithm that
is $R$-competitive with OPT in the off-line batch case in our
algorithm, then we get an on-line algorithm that is
$2R$-competitive with OPT. We give an inefficient algorithm that
accomplishes $R=1$, and several more interesting and efficient
algorithms with larger values of $R$.

\subsection{On-line Reduction to Off-line}
A \defn{single-time TAP} is a TAP where all tasks arrive at the
same time. In this subsection we show that algorithms that are
$R$-competitive with OPT on single-time TAPs can be used to make
an on-line algorithm that is $2R$-competitive with OPT.

Without loss of generality we consider TAPs where the cost ratio
$\pi(\tau)/\sigma(\tau) \in [1,p]$ for all tasks $\tau$; if
$\pi(\tau)/\sigma(\tau) < 1$ then the scheduler clearly should
never run $\tau$ in serial so we can replace the serial
implementation with the parallel implementation to get cost ratio
$1$, similarly, if $\pi(\tau)/\sigma(\tau) > p$ then the
scheduler should never run $\tau$ in parallel and we can replace
the parallel implementation with the serial implementation to get
cost ratio $p$.

A \defn{verge} time for our algorithm is a time where no
processors have work assigned to them, and there is at least one
ready task. We propose Algorithm~\ref{alg:bat}, which we call
\defn{$\bat_R$} (BAT is short for \enquote{batch}), as a
scheduling algorithm for the on-line problem. In this subsection
we analyze the competitive-ratio of $\bat_R$ assuming the
existence of an algorithm for $\oracle_R$; in
Subsection~\ref{subsec:offlineawaketime} we show that $\oracle_R$
can be computed, and in fact can be efficiently computed for $R \ge 2$.

\begin{algorithm}
  \caption{$\bat_R$}
  \label{alg:bat}
  \begin{algorithmic}
    \State Let $\oracle_R$ be an algorithm that is $R$-competitive with OPT on single-time TAPs 
    \If{verge time}
      \State schedule tasks as directed by $\oracle_R$
    \EndIf
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
  \label{thm:constant_competitive}
  $\bat_R$ is $2R$-competitive.
\end{theorem}
\begin{proof}
  Let an \defn{OPT-batch} be a maximal set of tasks done by OPT.
  Let a \defn{BAT-batch} be one of $\bat_R$'s batches. Call a BAT-batch
  \defn{internal} if it is a subset of an OPT-batch. Call a
  BAT-batch \defn{external} if it is not a subset of an
  OPT-batch. 

  Consider an OPT-batch $x$. Let $B_1$ be the union of the
  internal BAT-batches contained in $x$, let $B_2$ be the
  BAT-batch that overlaps with the end of $x$ (if such a batch
  exists), and let $B_3$ be the first BAT-batch that starts after
  $x$. Let $T_1$ be the time since the start of $x$ to the start
  of $B_2$ and let $T_2$ be the time from the start of $B_2$
  until the end of $x$. The amount of time that OPT spends on
  tasks from $x$ is clearly $T_1+T_2$. Now we bound how much time
  BAT spends on tasks from $x$. $B_1$ takes time $T_1$. $B_3$
  spends time at most $RT_2$ on tasks from $x$. $B_2$ spends time
  at most $R(T_1 + T_2)$ on tasks from $x$. Adding these times up
  we get that BAT spends at most $2R(T_1+T_2)$ time on the tasks
  from $x$. Adding this up across all OPT-batches $x$, gives that
  BAT is $2R$-competitive, as desired.
\end{proof}

\subsection{Off-line Approximation Algorithms}
\label{subsec:offlineawaketime}
In this Subsection we present algorithms for $\oracle_R$ for
various values of $R$.

First we give Algorithm~\ref{alg:frost}, which we call
FROST, for $\oracle_1$. 

\begin{algorithm}
  \caption{FROST (i.e. $\oracle_1$)}
  \label{alg:frost}
  \begin{algorithmic}
    \State \textbf{Input:} tasks $\tau_1,\ldots, \tau_n$ all with $t(\tau_i) = 0$
    \State \textbf{Output:} a way to schedule the tasks to
    processors $\rho_1, \ldots, \rho_p$ that achieves minimal awake time
    \State 
    \State $\text{minAwakeTime} \gets \infty$
    \State $\text{bestSchedule} \gets $ schedule everything in serial on $\rho_1$
    \For{$I \in \{0,1\}^n$} 
      \State $x \gets \sum_{i=1}^n I_i$
      \For{$J \in \{1, \ldots, p\}^x$}
        \State $j \gets 0$
        \For{$i \in \{1,2,\ldots, n\}$}
          \If{$I_i=1$}
            \State $j \gets j+1$
            \State schedule task $\tau_i$ in serial on $\rho_{J_j}$
          \EndIf
        \EndFor
        \State $m \gets \max_{\rho_i}(\work(\rho_i))$
        \State $w \gets \sum_{\rho_i} \paren{m - \work(\rho_i)}$
        \State $f \gets \sum_{\rho_i} (1-I_i)\pi(\rho_i)$
        \If{$f \ge w$}
          \State make $\rho_i$ have work $m + (f-w)/p$
        \Else 
          \State distribute $f$ units of work arbitrarily 
          \State among $\rho_i$ without increasing awake time
        \EndIf
        \If{$\text{awakeTime}(I, J) \le \text{minAwakeTime}$}
          \State{$\text{minAwakeTime} \gets \text{awakeTime}(I, J)$}
          \State{$\text{bestSchedule} \gets \text{schedule}(I, J)$}
        \EndIf
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

We remark that it is not obvious that an oracle for OPT can be
computed in finite time, even in the single-arrival-time case:
there are an uncountably infinite number of possible scheduling
strategies that OPT can choose from. The key insight to decrease
the search space to be finite is to notice that for any method of
distributing whichever tasks are chosen to run in serial, the
parallel tasks may as well be redistributed afterwords, so long
as doing so either results in not increasing the awake time, or
results in all tasks having identical amounts of work. We can
thus do a brute-force search over all the ways to assign some
tasks to run in serial and to run on specific processors, and
then put the parallel tasks on top \enquote{like frosting on a
cake}. We remark that the running time of FROST for $n$ tasks is
$\Theta(p^n)$, which is extremely large. Nevertheless the
existence of the algorithm is interesting. 

Now we prove that FROST is $1$-competitive with OPT.
\begin{lemma}[Frosting Lemma]
  \label{lem:frosting} 
  FROST is an oracle for OPT on single-time TAPs.
\end{lemma}
\begin{proof}
  Consider the configuration that OPT chooses. FROST considers
  a configuration of tasks with the same assignment of serial
  tasks at some point, because FROST brute force searches through
  all such configurations. For this configuration it is clearly
  impossible to achieve lower awake time than by spreading the
  parallel tasks in the frosting method, hence OPT's awake time
  is at least that of FROST.
\end{proof}

We remark that it is straightforward to extend FROST to a full
oracle for OPT. In particular, a full OPT oracle can be
constructed as follows: 

\begin{enumerate}
  \item Perform a brute force search over every
possible subset of the tasks to be the set of tasks scheduled in
parallel, and do a brute force search over every way to assign
the serial tasks to processors. 

\item Now consider the gaps, i.e. times
when all processors are idle under this current schedule. Add the
parallel tasks, scheduling them so as not to decrease the size of
any gaps unless it is impossible to do otherwise, in which case
the parallel tasks should be added to make all processors have
equal amounts of work extending into what used to be a gap.
\end{enumerate}

We now consider how to efficiently approximate OPT on
single-time TAPs. We propose Algorithm~\ref{alg:thresh}, which we
call THRESH, as a simple way to $2$-approximate OPT.

\begin{algorithm}
  \caption{THRESH}
  \label{alg:thresh}
  \begin{algorithmic}
    \State \textbf{Input:} tasks $\tau_1,\ldots, \tau_n$ with
    $\sigma(\tau_1) \ge \sigma(\tau_2)\ge \cdots \ge
    \sigma(\tau_n)$ all with $t(\tau_i) = 0$
    \State \textbf{Output:} a way to schedule the tasks to
    processors $\rho_1, \ldots, \rho_p$ that achieves awake time
    at most twice the optimal awake time.
    \State
    \State $w_\sigma \gets \sum_i \sigma(\tau_i)$
    \State $w_\pi \gets 0$
    \State $a \gets 0$, $i_* = 0$
    \For{$i \in [n]$}
      \State $w_\sigma \gets w_\sigma - \sigma(\tau_i)$
      \State $w_\pi \gets w_\pi + \pi(\tau_i)$
      \If{$i < n$}
        \State $a_0 = w_\sigma / p + \sigma(\tau_{i+1}) + w_\pi/p$
      \Else
        \State $a_0 = w_\pi/p$
      \EndIf
      \If{$a_0 \le a$}
        \State $i_* \gets i$
        \State $a \gets a_0$
      \EndIf
    \EndFor
    \State Schedule tasks $\tau_1, \ldots, \tau_{i_*}$ in
    parallel, distributing their work equally among the
    processors.
    \For{$k \in \{i_* + 1, \ldots, n\} $}
    \State Schedule $\tau_{k}$ in serial on processor $\rho_{1+(k \bmod p)}$
    \EndFor
  \end{algorithmic}
\end{algorithm}

THRESH schedules the $i_*$ tasks with largest serial work in
parallel, distributing their work equally, and schedules the rest
of the tasks in serial, sequentially giving out the tasks, for
the optimal value of $i_*$. Clearly THRESH only requires space
$\Theta(1)$ beyond the input to implement, and has running time
$\Theta(n)$, given that the input is pre-sorted.
We remark that a nice feature of THRESH is that it schedules
parallel tasks on a constant number of processors; we allow OPT
to schedule tasks on a variable number of processors (i.e.
unevenly distribute the work of a parallel task). It is nice to
not need to be able to unevenly distribute parallel tasks.

\begin{proposition}
THRESH is $2$-competitive with OPT on single-time TAPs.
\end{proposition}
\begin{proof}
Let $\tau_{i_0}$ be the task with the largest serial work that
OPT schedules in serial. Recalling that the tasks are sorted by
serial work, for all $i < i_0$ we have that OPT chooses to
schedule $\tau_i$ in parallel. Let $$w^{OPT}_\sigma = \sum_{i \ge
i_0} \sigma(\tau_i), w^{OPT}_\pi = \sum_{i < i_0} \pi(\tau_i).$$
OPT's awake time is obviously at least $$(w^{OPT}_\pi +
w^{OPT}_\sigma)/p.$$ OPT's awake time is also obviously at least
$\sigma(\tau_{i_0})$.

Without loss of generality we call the processor that gets the
most work, which used to be called $\rho_{1+(i_* \bmod p)}$,
$\rho_1$; in particular we circularly shift the labels of the
processors so that the work that $\rho_i$ gets is larger than the
work that $\rho_j$ gets for any $j > i$.
Think about the sequential thing.
Ignore $\sigma(\tau_{i_*})$. Then $\rho_i$ always has less work
than all the other processors. Of course in reality $\rho_1$ has
the most work of any processor.
So we have that all processors have serial work at most 
$$\sum_{i > i_*} \sigma(\tau_i)/p + \sigma(\tau_{i_*}).$$
Add on to that $$\sum_{i \le i_*} \pi(\tau_i)/p.$$

If $x \le a + b$, and $y \ge a, y \ge b$, then obviously $x \le
2y$. Thus, THRESH is $2$-competitive with FROST. 
\end{proof}

The assumption that the input is pre-sorted might not be
reasonable. If the tasks are not pre-sorted, then we must (at
least approximately) sort them for our algorithm to achieve a
good competitive-ratio. We now discuss several approaches for
doing this sorting which will involve trade-offs in competitive
ratio, running time, and space.

If we simply use heap sort to sort the tasks, then we
will have increased running time of $\Theta(n \log n)$, but will
retain our constant-space requirement and will still have
competitive ratio $2$.

If we use radix sort then we can possibly achieve better
running time. 

If the tasks can be assumed to
have serial works that can only take on $O(1)$ possible
values, then the radix sort can be performed in-place in linear
time, which results in a linear-time $2$-competitive algorithm. 

If the serial works can take on more than $\Omega(1)$ possible
values, we can still achieve better than linearithmic running
time by sacrificing competitive ratio. Let $1$ be the largest
serial work of any task. We sort the tasks based on which bucket
$[1/2^i, 1/2^{i-1}]$ the task's serial work falls in, --- i.e.
rounding their serial works ---  except putting tasks with work
less than $1/2^{\lg (np)}$ in a single bucket. Now there are only
$\log (np)$ distinct keys, so sorting can be done faster. 

If we are willing to spend $\Theta(n + \log (np))$
extra space, counting sort can sort the tasks (based on the
rounded keys) in time $\Theta(n+\log(np))$. 
If we want to still only use $O(1)$ space, then by
sequentially partitioning on each bit of the key's ranks we get
an in-place algorithm with running time $O(n \log \log (np))$.

Since the tasks are only approximately sorted we cannot
guarantee competitive ratio $2$ with this method. However, we can
guarantee that we are $8$-competitive: with the exception of the
tasks with work smaller than $1/(np)$ we have increased each
task's size by at most a factor of $2$; increasing the serial
works of all tasks by a factor of $2$ obviously doubles the awake
time that we achieve. Now consider the tasks with works at most
$1/(np)$. The sum of their works is at most $1/p$, and the
largest task has work $1$, and hence takes time at least $1/p$ to
run.

Another interesting assumption to remove is the assumption that
we know the works of the parallel implementations of tasks: it
turns out that, if we are willing to sacrifice a constant-factor
in the competitive ratio, we do not even need to know the
parallel works of the tasks before running them! Now we give an
algorithm that is $4$-competitive with OPT, which \emph{does not
use the size of the parallel implementations of tasks}: it uses
the size of the serial tasks to decide which tasks to run, and
then is told how long parallel tasks took to run after running
them. We present this algorithm in Algorithm~\ref{alg:run}, which
we call \defn{RUN}. We remark that the same techniques discussed
above for removing the assumption that tasks are pre-sorted in
THRESH could be used in RUN.

\begin{algorithm}
  \caption{RUN}
  \label{alg:run}
  \begin{algorithmic}
    \State \textbf{Input: } a list of tasks $\tau_1, \tau_2,
    \ldots, \tau_n$, sorted in descending order by
    $\sigma(\tau_i).$
    \State \textbf{Output: } a schedule that is $4$-competitive
    with the schedule FROST would make
    \State 
    \State $w_\pi \gets 0$
    \State $i \gets 1$
    \While{$i \le n$}
      \If{$w_\pi > \sigma(\tau_i)$}
        \State \textbf{break}
      \EndIf
      \State Run task $\tau_i$ in parallel
      \State $w_\pi \gets w_\pi + \pi(\tau_i)/p$
      \State $i \gets i+1$
    \EndWhile
    \While{$i \le n$}
      \State Schedule $\tau_i$ in serial on $\rho_{1+(i \bmod p)}$
      \State $i \gets i+1$
    \EndWhile
  \end{algorithmic}
\end{algorithm}

Note that RUN accesses $\pi(\tau_i)$ only after running $\tau_i$
in parallel, after which it could of course have measured
$\pi(\tau_i)$. RUN and THRESH give very similar strategies.
RUN's strategy can be slightly worse than THRESH's though, so RUN
has a slightly worse competitive ratio of $4$ versus OPT.

\begin{proposition}
  RUN is $4$-competitive with OPT.
\end{proposition}
\begin{proof}
  Let $\tau_{i_*}$ be the task with the largest serial work that
  RUN schedules in serial.
  Since $$\sum_{i < i_*} \pi(\tau_i) / p > \sigma(\tau_{i_*})$$
  it is clearly impossible for OPT to achieve awake time better
  than $\sigma(\tau_{i_*})$: if OPT schedules everything with
  serial work at least $\sigma(\tau_{i_*})$ in parallel OPT's
  awake time is at least $\sigma(\tau_{i_*})$, and if OPT
  schedules any tasks with serial work at least
  $\sigma(\tau_{i_*})$ in serial OPT's awake time is at least
  $\sigma(\tau_{i_*})$.

  The running time of OPT is also of course at least 
  $$\sum_{i=1}^n \sigma(\tau_i)/p.$$

  The running time of the parallel tasks for RUN is clearly at
  most $2\sigma(\tau_{i_*})$.

  The running time for the parallel tasks of RUN is, by the same
  analysis presented in the analysis of THRESH, at most 
  $$\sum_{i > i_*}\sigma(\tau_i) / p + \sigma(\tau_{i_*}).$$

  Combined we have the upper bound of 
  $$\sum_{i > i_*}\sigma(\tau_i) / p + 3\sigma(\tau_{i_*})$$ 
  on the awake time of RUN. Using our lower bounds on the awake
  time of OPT we have that RUN is $4$-competitive.

\end{proof}

\subsection{Lower Bound against Deterministic Algorithms}
In this Subsection we prove several impossibility results, which
show that we cannot hope to substantially improve our algorithms.

We can specify a TAP in a table with a list of which tasks arrive
at each time; we use the compact notation $(\sigma, \pi)\times m$
to denote $m$ identical tasks with serial works $\sigma$ and
parallel works $\pi$. It is clear that $\bat_1$ is not
$(2-\epsilon)$-competitive for any $\epsilon > 0$; consider, for
example the TAP given in Table~\ref{tab:2minusEps}, on which BAT
achieves awake time $2$ and OPT achieves awake time
$1+\varepsilon$.

\begin{table}[H]
\caption{}
\label{tab:2minusEps}
\centering
\begin{tabular}{|l|l|}
\hline
time & tasks                    \\ \hline
$0$  & $(1,p) \times p/2$       \\ \hline
$\varepsilon$  & $(1, p) \times p/2$ \\ \hline
\end{tabular}
\end{table}

We might hope to achieve a $(1+\epsilon)$-competitive scheduling
algorithm for the on-line scheduling problem, which would be
better than $\bat_1$. However, we show that it is impossible
for an off-line deterministic scheduler to get a competitive
ratio lower than $1.25$, even using preemption. That is, we show
that for any deterministic algorithm ALG there is some input on
which ALG has awake time at least $1.25$ times greater than OPT. 

In Table~\ref{tab:lowerboundFork1} and
Table~\ref{tab:lowerboundFork2} we specify two TAPs.

\begin{table}[H]
\caption{}
\label{tab:lowerboundFork1}
\centering
\begin{tabular}{|l|l|}
\hline
time & tasks                    \\ \hline
$0$  & $(4, 2p) \times 1$       \\ \hline
$1$  & $(3, 3p) \times (p-1)$ \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\caption{}
\label{tab:lowerboundFork2}
\centering
\begin{tabular}{|l|l|}
\hline
time & tasks                    \\ \hline
$0$  & $(4, 2p) \times 1$       \\ \hline
\end{tabular}
\end{table}

Consider an arbitrary deterministic scheduling algorithm. If at
time $0$ the arriving tasks are $(4, 2p)\times 1$ (i.e. a single
task arrives, with serial work $4$ and parallel work $2p$) then
the scheduler has two options: it can schedule this task in
serial, or in parallel.

If no further tasks arrive, i.e. the task schedule is from
Table~\ref{tab:lowerboundFork2} then OPT would have awake time
$2$ by distributing the tasks work equally amongst all
processors, whereas a scheduler that ran the task in serial for
all of the time that it was running the task during the first
second after the task arrived would have awake time at least $3$.
In this case the competitive ratio of the algorithm is at least $1.5$.

On the other hand, the algorithm could decide to not run the task
in serial for any time during the first second after the task
arrives. In this case, if
and it turns out that the task schedule is from
Table~\ref{tab:lowerboundFork1}, then the algorithm has again
acted sub-optimally. In particular, for the schedule given in
Table~\ref{tab:lowerboundFork1}, OPT schedules the task that
arrives at time $0$ in serial, and then schedules all the tasks
that arrive at time $1$ in serial as well, and hence achieves
awake time $4$. On the other hand, the awake time of an algorithm
that did not schedule the task that arrived at time $0$ in
serial is at least $5$: such a scheduler may either choose at
time $1$ to cancel the task from time $0$ and run it in serial,
or the scheduler may choose to let the parallel implementation
finish running. In this case the competitive ratio of the
algorithm is $5/4$.

Hence it is impossible for any deterministic algorithm to achieve
a competitive ratio of lower than $1.25$.

We remark that the numbers in this argument can clearly be
optimized, to give an improved lower bound of about $1.36$ on
competitive ratio. As this is asymptotically not interesting, and
much messier, we present the argument with cleaner numbers.

\subsection{Lower Bound against Randomized Algorithms}
Often randomized algorithms can achieve improvements over
deterministic algorithms, so we for example hope for a
$(1+\epsilon)$-competitive randomized on-line scheduling
algorithm that succeeds on any input with high probability.
However, in this subsection we show that this is impossible.

In particular, we demonstrate a lower-bound of $1.0625$ on the
competitive ratio of any randomized off-line algorithm.

Recall the TAPs from Table~\ref{tab:lowerboundFork1} and
Table~\ref{tab:lowerboundFork2}; we will use these as sub-parts
of our the TAP that we build to be adversarial for a randomized
algorithm. 

Fix some off-line randomized algorithm RAND. We say that an input
TAP is \defn{bad} for RAND if with high probability RAND's awake
time on TAP is at least $1.0625$ times that of OPT.
We construct a class of TAPs, and show that some of the TAPs in
this class must be bad for RAND.

Let $\mathcal{T}_{I}$, for some some binary string $I$, be the
TAP consisting of the TAP from Table~\ref{tab:lowerboundFork1} at
time $10i$ if $I_i = 1$ and the TAP from
Table~\ref{tab:lowerboundFork2} if $I_i = 0$. 

Consider a $I$ chosen uniformly at random from $\{0,1\}^m$ for
some parameter $m$.
On each sub-tap RAND has at most a $1/2$ chance of acting as OPT
does, and at least a $1/2$ chance of acting sub-optimally, in
particular, from our analysis above showing that any deterministic
algorithm has competitive ratio at least $1.25$ on at least one
of these inputs, RAND has at least a $1/2$ chance of this
happening.
By a Chernoff Bound, with probability at least
$1-e^{-\Omega(m)}$, on at least $1/4$ of the sub-taps RAND has
competitive ratio at least $1.25$. Since there is no overlap, by
design, of the sub-taps (by spacing them out), this means that
overall the competitive ratio of RAND is at least $1\cdot 3/4 +
1.25 \cdot 1/4 = 1.0625.$

Note that the number of tasks in such a TAP is less than $mp$, so
$n \le mp$, and thus $m \ge n/p$.
Hence our result that holds with high probability in $m$ holds
with high probability in $n/p$ too.
Of course $n\gg p$ so this is pretty decent.

Because a randomly chosen TAP from this class of TAPs is bad for
RAND with high probability in $n/p$, by the probabilistic method
there is at least one TAP in this class of TAPs that is bad for
RAND. 



\section{A Deterministic Scheduling Algorithm}
In this section we present a simple deterministic scheduling
algorithm that does not use preemption. We show that this
algorithm is $2$-competitive with OPT.

First we note that without loss of generality we may consider
TAPs where the cost ratio $\pi(\tau)/\sigma(\tau)$ for any task
$\tau$ is in $[1,p]$; if $\pi(\tau)/\sigma(\tau) < 1$, i.e. the
parallel implementation has lower work than the serial
implementation, then the scheduler clearly should never use the
serial implementation of this algorithm, so we can replace the
serial implementation with the parallel implementation and hence
get cost ratio $1$, similarly, if $\pi(\tau)/\sigma(\tau) > p$
then the scheduler should never run the parallel task and we can
replace the parallel implementation of the task with the serial
implementation to get cost ratio $p$.

We say that a time is a \defn{verge} time for our algorithm if at
this time no processors are performing tasks and there is at
least one ready task.

We propose Algorithm~\ref{alg:gr}, which we call \defn{GR}
(GR is short for \enquote{greedy}), as a scheduling algorithm.

\begin{algorithm}
  \caption{GR}
  \label{alg:gr}
  \begin{algorithmic}
    \While{True}
      \If{verge time}
        \State schedule tasks as directed by ORACLE-1 
      \EndIf
    \EndWhile
  \end{algorithmic}
\end{algorithm}

ORACLE-1 is an algorithm that yields a schedule achieveing
minimal awake time, i.e. the same awake time as OPT, on a TAP
where all tasks arrive at the same time. In
Theorem~\ref{thm:cakefrosting} we establish that ORACLE-1, which
we specify in Algorithm~\ref{alg:opt_oracle}, is an oracle for
OPT on TAPs where all tasks arrive at a single time, hence
showing that GR can actually be computed. Before analyzing
ORACLE-1, which is a quite involved process, we analyze the
competitive ratio of GR assuming the existence of ORACLE-1. 

Consider a TAP $\mathcal{T}$. Let $\ell$ be the number of verge
times for $\mathcal{T}$; note that $\ell \le n$ which in
particular is finite. Let $t_i$ be the $i$-th time that is a
verge time, let $q_i$ be the number of ready tasks for GR at
time $t_i$. Let $T^{ALG}(q_1, \ldots, q_{\ell'})$ denote the
awake time of a scheduling algorithm ALG on the truncation of the
TAP $\mathcal{T}$ that only consists of tasks arriving at times
before $t_{\ell'}$.

By construction we have
\begin{equation}
  \label{eq:same_single}
  T^{OPT}(q) = T^{GR}(q).
\end{equation}
We remark GR \enquote{locally} schedules optimally, which is why
we refer to GR as \enquote{greedy}. 

An \defn{ALG-gap} is an interval of time $I$ of non-zero length where for
all times in the interior of $I$ ALG has completed every
task that has arrived thus far. Additionally for an interval to
be an ALG-gap the interval must contain no other intervals which
are also ALG-gaps (i.e. it is a \enquote{maximal} interval
satisfying our conditions).
We say that a TAP is \defn{ALG-gap-free} if it contains no ALG-gaps.

Now we prove an important property of OPT.
\begin{claim}
  \label{clm:OPT_finishes_first}
  If there is a scheduling algorithm ALG that completes all tasks by
  time $t_*$ then OPT finishes all tasks by time $t_*$.
\end{claim}
\begin{proof}
  Say that ALG completes all tasks by time $t_*$. Let $t_0 < t_*$
  be the most recent time that OPT has completed all tasks that
  arrive before time $t_0$. If OPT has not finished all tasks by
  time $t_*$ then it was acting sub-optimally, as it could steal
  the strategy that ALG used on $[t_0, t_*]$ to achieve lower
  awake time. In particular, for any tasks that arrive in $[t_0,
  t_*]$ OPT could schedule them as ALG schedules them. We remark
  that OPT should not steal all of ALG. 
\end{proof}
As an immediate consequence of Claim~\ref{clm:OPT_finishes_first}
we have that any ALG-gap is a subset of an OPT-gap.

Decomposing TAPs into gap-free subsets of the TAP is very useful.
Part of the reason for this is the following fact:
\begin{claim}
  \label{clm:just_consider_gapless}
  If an algorithm ALG achieves competitive ratio $r$ on
  ALG-gap-free TAPs, then ALG achieves 
  competitive ratio $r$ on arbitrary TAPs.
\end{claim}
\begin{proof}
  We partition the tasks based on arrival time, splitting the
  tasks on the ALG-gaps. That is, we split the tasks into groups
  so that two tasks $\tau_i, \tau_j$ are in the same group if and
  only if there are no gaps in between the arrival times of
  $\tau_i$ and $\tau_j$.
  We can define an interval of time $I_i$ for each of these
  ALG-gap-free subsets of the TAP, where $I_i$ is defined so that
  all tasks in the $i$-th group start and finish at times
  contained in the interval $I_i$.

  Let $T_{I_i}^{OPT}$ and $T_{I_i}^{ALG}$ denote the awake time
  of OPT and ALG on interval $I_i$. Because $I_i$ is ALG-gap-free
  we have $T^{ALG} = \sum_{i} T^{ALG}_{I_i}$.
  Further, recall that by Claim~\ref{clm:OPT_finishes_first} any
  ALG-gap is also an OPT-gap, so
  $T^{OPT} = \sum_{i} T_{I_i}^{OPT}$. 
  Hence from our assumption that ALG is $r$-competitive on
  gap-free TAPs, such as the subset of the TAP on the interval
  $I_i$, we have $T_{I_i}^{ALG} \le r\cdot T_{I_i}^{OPT}$ for
  all $i$. Summing we get $T^{ALG} \le r\cdot T^{OPT}$, as desired.
  
\end{proof}

By Claim~\ref{clm:just_consider_gapless}, in order to bound GR's
competitive ratio, it suffices to consider TAPs
without GR-gaps. Note however that a TAP without
GR-gaps could still have OPT-gaps.

We conclude our analysis of the competitive ratio of GR in
Proposition~\ref{prop:2competitive} with an inductive argument on
the number of OPT-gaps in the TAP.
First we establish the base case for the argument in
Claim~\ref{clm:no_optgaps}: we consider
GR's competitive ratio on a TAP without OPT-gaps.  

\begin{claim}
  \label{clm:no_optgaps}
  GR is $2$-competitive on OPT-gap-free TAPs.
\end{claim}
\begin{proof}
  For an OPT-gap-free TAP we must have
  \begin{equation}
    \label{eq:opt_isnt_so_much_better}
    T^{OPT}(q_1, \ldots, q_{\ell}) \ge T^{GR}(q_1, \ldots, q_{\ell-1}).
  \end{equation}
  Because GR finishes all $q_{i}$ tasks that arrive at time $t_i$
  by time $t_{i+1}$ we can actually always decompose
  $T^{GR}(q_1, \ldots, q_\ell)$ as 
  \begin{equation}
    \label{eq:decomposeGR}
    T^{GR}(q_1, \ldots, q_\ell) = \sum_{i=1}^\ell T^{GR}(q_i).
  \end{equation}
  By Equation~\eqref{eq:decomposeGR}, and
  Equation~\eqref{eq:same_single} we thus have 
  \begin{equation}
    \label{eq:decompose_rearanged}
    T^{GR}(q_1, \ldots, q_\ell) = T^{GR}(q_1, \ldots, q_{\ell-1}) + T^{OPT}(q_\ell).
  \end{equation}

  Hence by Equation~\eqref{eq:opt_isnt_so_much_better} and
  Equation~\eqref{eq:decompose_rearanged} we have
  \begin{align*}
    T^{GR}(q_1, \ldots, q_\ell) &\le T^{OPT}(q_1, \ldots, q_\ell) + T^{OPT}(q_\ell)\\
                                   &\le 2T^{OPT}(q_1, \ldots, q_\ell),
  \end{align*}
  as desired.
\end{proof}

\begin{proposition}
  \label{prop:2competitive}
  GR is $2$-competitive.
\end{proposition}
\begin{proof}
  The proof is by strong induction on the number of OPT-gaps. 
  The base case of our induction is established in
  Claim~\ref{clm:no_optgaps}, which says that if there are $0$
  OPT-gaps then GR is $2$-competitive. 

  Consider a TAP that has more than $0$ OPT gaps;
  say that its first OPT-gap starts at time $t_*$.
  Let $j$ be the largest index such that verge time $t_j <
  t_*$.

  Using our inductive hypothesis we have:
  \begin{align*}
  &T^{OPT}(q_1, \ldots, q_\ell) \\
  &\ge T^{OPT}(q_1, \ldots, q_j) + T^{OPT}(q_{j+1}, \ldots, q_{\ell})\\
  &\ge \frac{1}{2}\paren{T^{GR}(q_1, \ldots, q_j) + T^{GR}(q_{j+1}, \ldots, q_{\ell})}\\
  &=\frac{1}{2} T^{GR}(q_1, \ldots, q_\ell).
  \end{align*}

\end{proof}

Now we analyze Algorithm~\ref{alg:opt_oracle}, which we call
ORACLE-1. 

\todo{simpler description of frosting}
\begin{algorithm}
  \caption{Single-time-TAP OPT Oracle}
  \label{alg:opt_oracle}
  \begin{algorithmic}
    \State \textbf{Input:} tasks $\tau_1,\ldots, \tau_n$ all with $t(\tau_i) = 0$
    \State \textbf{Output:} a way to schedule the tasks to
    processors $\rho_1, \ldots, \rho_p$ that achieves minimal awake time
    \State 
    \State $\text{minAwakeTime} \gets \infty$
    \State $\text{bestSchedule} \gets $ schedule everything in serial on $\rho_1$
    \For{$I \in \{0,1\}^n$} 
      \State $x \gets \sum_{i=1}^n I_i$
      \For{$J \in \{1, \ldots, n\}^x$}
        \State $j \gets 0$
        \For{$i \in \{1,2,\ldots, n\}$}
          \If{$I_i=1$}
            \State $j \gets j+1$
            \State schedule task $\tau_i$ in serial on $\rho_{J_j}$
          \EndIf
        \EndFor
        \For{$i \in \{1,2,\ldots, n\}$}
          \If{$I_i=0$}
            \State $f \gets \pi(\tau_i)$
            \While{$f > 0$}
              \State $b_1\gets \min_{\rho_i}(\text{work}(\rho_i))$
              \State $B_1 \gets \{\rho_i | \text{work}(\rho_i)=b_1\}$
              \State $b_2\gets \inf_{\rho_i\not\in B}(\text{work}(\rho_i))$
              \State $\Delta_f \gets \min(f/|B_1|, b_2-b_1)$
              \State give fill $\Delta_f$ to all $\rho_i \in B_1$
              \State $f \gets f - \Delta_f$
            \EndWhile
          \EndIf
        \EndFor
        \If{$\text{awakeTime}(I, J) \le \text{minAwakeTime}$}
          \State{$\text{minAwakeTime} \gets \text{awakeTime}(I, J)$}
          \State{$\text{bestSchedule} \gets \text{schedule}(I, J)$}
        \EndIf
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

The algorithm is a straightforward brute-force approach to
computing what to do. The algorithm tries every possible way of
assigning some tasks to be run in parallel and others to be run
in serial, and for each of these the algorithm tries every
possible way to arrange the serial tasks, and for each of these
the algorithm then \defn{frosts} the configuration with parallel
tasks, meaning, the algorithm distributes the parallel tasks so
as to minimally increase awake time.

We remark that the running time of ORACLE-1 for $n$ tasks is at
least $\sum_{i=1}^n n^{\binom{n}{i}} \ge 2^{2^n},$ which is
pretty big. Nevertheless the existence of our algorithm is very
interesting, and the running time can probably re reduced.

We now prove that ORACLE-1 actually does compute a schedule with
awake time the same as that of OPT.
\begin{theorem}[Cake Frosting Theorem]
  \label{thm:cakefrosting} 
  ORACLE-1 is an oracle for OPT on TAPs where all tasks arrive at a single time.
\end{theorem}
\begin{proof}
  Consider a TAP where all tasks arrive at a single time. We
  claim that the awake time achieved by OPT is not better than
  the awake time achieved by ORACLE-1, or equivalently GR (which
  in this case is the same as ORACLE-1).

  \todo{make this not specific to the single-task case}
  Say OPT schedules $x$ of the $q$ tasks in serial, and
  schedules the remaining $q-x$ in parallel.
  Say that an optimal assignment of the tasks, under the constraint
  that $x$ of the tasks are scheduled in serial and $q-x$ are
  scheduled in parallel, achieves awake time $M$.

  We claim that there must exist an optimal assignment of tasks
  such that each processor is assigned either $\floor{x/p}$ or
  $\ceil{x/p}$ serial tasks. To prove this, we start from some
  optimal assignment, and modify it in such a way as to make the
  configuration \enquote{closer} to our desired configuration.
  Let $n_\sigma(\rho_i)$ be the number of serial tasks scheduled
  on processor $\rho_i$.

  To formalize a notion of \enquote{closeness} we define a
  potential function $\phi$ of the assignment $S$ of tasks:
  $$\phi(S) = \sum_{i} \min(|n_\sigma(\rho_i)-\floor{x/p}|,
  |n_\sigma(\rho_i)-\ceil{x/p}|.$$ Note that $\phi(S)$ is
  non-negative. We desire a configuration with $\phi(S) = 0$.
  Consider a configuration of tasks achieving awake time $M$ with
  $\phi(S) > 0$. We first apply the following procedure to the
  configuration: while the difference between the maximum work
  assigned to a processor and the minimum work assigned to a
  processor is more than $1$ swap $1$ unit of work from a
  processor with the maximum amount of work to a processor with
  the minimum amount of work. This swap cannot increase the range
  of works assigned to processors, and the process must
  eventually terminate. Now, while $\phi(S) > 0$ there must
  exists $\rho_i, \rho_j$ with $n_\sigma(\rho_i) < \floor{x/p}$
  and $n_\sigma(\rho_j) > \floor{x/p}$ or there exists $\rho_i,
  \rho_j$ with $n_\sigma(\rho_i) < \ceil{x/p}$ and
  $n_\sigma(\rho_j) > \ceil{x/p}$. By construction the range of
  the works is at most $1$; hence $\rho_i$ has at least $1$ unit
  of parallel work to have work within $1$ of $\rho_j$ despite
  having at least $2$ fewer serial tasks than $\rho_j$. Then
  $\rho_i$ gives this $1$ unit of parallel work to $\rho_j$, and
  in exchange $\rho_j$ gives $\rho_i$ a serial task. This
  swapping operation decreases $\phi(S)$ by exactly $1$, and does
  not change the amount of work assigned to each processor, which
  importantly means that the swap does not increase the range of
  the works. Hence, we can repeat this swapping process to
  eventually achieve a configuration with awake time $M$ where
  each processor has $\floor{x/p}$ or $\ceil{x/p}$ serial tasks.

  If $x\bmod p = 0$ then the awake time of OPT is clearly at
  least 
  \begin{equation} \label{eq:case_xmodp0}
    x/p + (q-x)k/p.
  \end{equation}
  Note that if $x$ increases by $p$ then \eqref{eq:case_xmodp0}
  changes by $1-k < 0$. That is, \eqref{eq:case_xmodp0} is
  monotonically decreasing in $x$, and is thus minimized by
  setting $x = p\floor{q/p}$ and hence getting $q-x = q\bmod p$.
  This gives awake time $\floor{q/p} + (q\bmod p)k/p \ge T(q)$.

  Now we consider the case where $x\bmod p \neq 0$. Here $(p -
  (x\bmod p))/k$ tasks can be added in serial without
  increasing the work at all. Thus the awake time is at least
  \begin{equation} \label{eq:case_xmodpnot0}
    \ceil{x/p} + \frac{k}{p}\paren{\max\paren{0, q-x - \frac{p-(x\bmod p)}{k}}}.
  \end{equation}

  Consider when the $\max$ in \eqref{eq:case_xmodpnot0} yields $0$. 
  For this to happen we must have
  $$q-x \le \frac{p-x\bmod p}{k}.$$
  As $p-x\bmod p \le p$ we get the following bound on $q-x$:
  $$q-x \le p/k,$$
  which must be met in order for the $\max$ to yield $0$.
  Clearly as $q \ge x \ge q-p/k$, $\floor{x/p} \ge \floor{q/p}-1$; we
  claim that this inequality is actually strict. Imagine that
  $\floor{x/p} = \floor{q/p}-1$. Because the $\max$ expression
  yields $0$, meaning that the parallel tasks add nothing to the
  awake time, if all processors only have $\floor{q/p}-1$ or
  $\floor{q/p} < q/p$ serial tasks, then the total work is at
  most $\floor{q/p} < q/p$ which is impossible: the total work in
  the system must be at least $q$ and it can be distributed in
  the best case perfectly equally which makes $q/p$ as a lower
  bound on the time to complete $q$ tasks. Hence $\floor{x/p} = \floor{q/p}$. 
  But then the awake time is at least $\ceil{x/p} = \floor{q/p} + 1 \ge T(q)$.

  Now we consider the case when the $\max$ in
  \eqref{eq:case_xmodpnot0} yields some positive number.
  Note that if $x$ increases by $p$ (but $x$ is still
  sufficiently small so that the $\max$ yields a positive number)
  then \eqref{eq:case_xmodpnot0} changes by $1-k < 0$.
  Further, if $x$ increases by $1$ (but $x$ is still
  sufficiently small so that the $\max$ yields a positive number) without $\ceil{x/p}$
  changing, then \eqref{eq:case_xmodpnot0} changes by $(k/p)(1/k
  -1) < 0$. That is, \eqref{eq:case_xmodpnot0} is monotonically
  decreasing in $x$. Hence we still have that the awake time is at least $T(q).$

  We have considered all cases, and shown that no matter what
  choice of $x$ OPT makes, and no matter how OPT schedules given
  that choice of $x$, OPT must incur awake time at least $T(q)$,
  as desired.
\end{proof}



\section{Introduction}
\label{sec:intro}
\subsection{Problem Specification}
A parallel algorithm is said to be \defn{work-efficient} if the
work of the parallel algorithm is the same as the work of a
serial algorithm for the same problem. Most implementations of
parallel algorithms are not work-efficient, often having work
that is a constant factor greater, or even asymptotically
greater, than the work of the serial algorithm for the problem.

In the \defn{Serial-Parallel Scheduling Problem} we have to
perform $n$ tasks $\tau_1, \ldots, \tau_n$ ($n$ unknown ahead of
time). We have $p$ processors $\rho_1, \ldots, \rho_p$. Each task
$\tau_i$ has a parallel implementation with work $\pi(\tau_i)$ and a
serial implementation with work $\sigma(\tau_i)$. The tasks will become
available at some times $t(\tau_1), \ldots, t(\tau_n)$. 
The sequence of tasks with their associated parallel and serial
implementations works and with their associated arrival times is
called a \defn{task arrival plan} or \defn{TAP} for short.

The scheduler maintains a set of \defn{ready} tasks, which are
tasks that have become available but are not currently being run
on any processor. At time $t(\tau_i)$ task $\tau_i$ is added to
the set of ready tasks. At any time the scheduler can decide to
schedule some (not already running) ready task, and can choose
whether to run the task in serial, in which case the scheduler
must choose a single processor to run the task on, or in
parallel, in which case the scheduler can distribute the task's
work arbitrarily among the processors. Intuitively, if there are
many ready tasks then the scheduler should run the serial
implementations of the tasks because the scheduler can achieve
parallelism across the tasks. On the other hand, if there are not
very many ready tasks it is probably better for the scheduler to
run the parallel versions of the tasks --- even though they are
possibly not work efficient, i.e. $\pi(\tau) > \sigma(\tau)$ ---
because by so doing at least the scheduler can achieve
parallelism within tasks.

Let the \defn{awake time} of the scheduler be the duration of
time over which the scheduler has unfinished tasks. One natural
goal for the scheduler is to minimize awake time. We measure how
well the scheduler is able to minimize its awake time by
comparing its awake time to the awake time of the optimal
off-line strategy, which we will denote OPT. Note that OPT is
able to see the whole sequence of tasks in advance. The
\defn{competitive ratio} of a scheduler is the ratio of its awake
time to the awake time of OPT on the same input.

% Another interesting metric to consider is \defn{mean response
% time}, the average over the tasks of the time between when the
% task arrives and when the task is finished. Again we can consider
% the competitive ratio of a scheduler relative to this metric.

\subsection{Problem Motivation}
Data-centers often get heterogeneous tasks. Being able to schedule
them efficiently is a fundamental problem. 

In the CILK programming language whenever a function is called we
could let the scheduler choose between a serial and a parallel
implementation of a task. 

\subsection{Related Work}
Shortest Job First (SJF) is a pretty common idea for minimum response time. 
An algorithm called Shortest Remaining Processing Time (SRPT),
and its variants are often useful for minimizing metrics like mean
response time.

In \cite{bb20}, Berg et al study a related problem:
many heterogeneous tasks come in, some which are elastic and
exhibit perfect linear scaling of performance and some which are
inelastic which must be run on a single processor, according to
some stochastic assumptions, and they aim to minimize mean
response time. They show that for some parameter settings the
strategy \enquote{Inelastic First} is optimal.

In \cite{is16} Im et al exhibit an algorithm keeps the average
flow time small. 

In \cite{ga12} Gupta et al prove some impossibility results about
a problem somewhat similar to our problem.

Clearly related problems are widely studied.
Our problem is novel however, and interesting.

\subsection{Results}
We show that, very surprisingly, the off-line problem can
essentially be reduced to the case where all tasks are available
from the start, a setting in which the off-line and on-line
algorithms are of course the same. In particular, we give a
simple deterministic $2$-competitive off-line scheduling
algorithm, that does not even need to use preemption! The
$2$-competitive algorithm relies on solving a special case of the
off-line problem where all tasks arrive at the same time. We give
a $3$-approximation algorithm for the single-arrival-time
off-line problem with running time $O(n)$.

We also prove several impossibility results.
We show that no deterministic scheduler can have a
competitive ratio smaller than $1.25$ in general.
Even with randomization, we show that for any randomized
algorithm there is some input on which the algorithm achieves
competitive ratio at least $1.0625$ with high probability.

We also consider a generalization of the scheduling problem where
tasks can have dependencies. Here we show XXX.

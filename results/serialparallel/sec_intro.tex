\section{Introduction}
\label{sec:intro}
\subsection{Problem Specification}
A parallel algorithm is said to be \defn{work-efficient} if the
work of the parallel algorithm is the same as the work of a
serial algorithm for the same problem. Most implementations of
parallel algorithms are not work-efficient, often having work
that is a constant factor greater, or even asymptotically
greater, than the work of the serial algorithm for the problem.

In the \defn{Serial-Parallel Scheduling Problem} we have to
perform $n$ tasks $\tau_1, \ldots, \tau_n$ ($n$ unknown ahead of
time). We have $p$ processors $\rho_1, \ldots, \rho_p$. Each task
$\tau_i$ has a parallel implementation with work $\pi(\tau_i)$ and a
serial implementation with work $\sigma(\tau_i)$. The tasks will become
available at some times $t(\tau_1), \ldots, t(\tau_n)$. 
The set of tasks with their associated parallel and serial
implementation's works and with their associated arrival times is
called a \defn{task arrival plan} or \defn{TAP} for short.

The scheduler maintains a set of \defn{ready} tasks, which are
tasks that have become available but are not currently being run
on any processor. At time $t(\tau_i)$ task $\tau_i$ is added to
the set of ready tasks. At any time the scheduler can decide to
schedule some (not already running) ready task, and can choose
whether to run the task in serial, in which case the scheduler
must choose a single processor to run the task on, or in
parallel, in which case the scheduler can distribute the task's
work arbitrarily among the processors. Intuitively, if there are
many ready tasks then the scheduler should run the serial
implementations of the tasks because the scheduler can achieve
parallelism across the tasks. On the other hand, if there are not
very many ready tasks it is probably better for the scheduler to
run the parallel versions of the tasks --- even though they are
possibly not work efficient, i.e. $\pi(\tau) > \sigma(\tau)$ ---
because by so doing at least the scheduler can achieve
parallelism within tasks.

We also consider a generalization of the Serial-Parallel
Scheduling Problem to the case where the task arrival times are
not fixed, but rather some tasks may become available only after
the completion of other tasks. Put another way, we consider a
version of the problem where the tasks have dependencies. We
refer to the set of tasks along with their associated parallel
and serial implementation's works and their associated arrival
times or dependency (i.e. what task they spawn after) as a
\defn{DTAP} (D stands for dependency).

Let the \defn{awake time} of the scheduler be the duration of
time over which the scheduler has unfinished tasks. One natural
goal for the scheduler is to minimize awake time. We measure how
well the scheduler is able to minimize its awake time by
comparing its awake time to the awake time of the optimal
off-line strategy, which we will denote OPT. Note that OPT is
able to see the whole sequence of tasks in advance. The
\defn{competitive ratio} of a scheduler is the ratio of its awake
time to the awake time of OPT on the same input.

% Another interesting metric to consider is \defn{mean response
% time}, the average over the tasks of the time between when the
% task arrives and when the task is finished. Again we can consider
% the competitive ratio of a scheduler relative to this metric.

\subsection{Notation}
We use $[n]$ to denote $\{1,2,\ldots, n\}.$

\subsection{Problem Motivation}
Data-centers often get heterogeneous tasks. Being able to schedule
them efficiently is a fundamental problem. 

In the CILK programming language whenever a function is called we
could let the scheduler choose between a serial and a parallel
implementation of a task. 

\subsection{Related Work}
Shortest Job First (SJF) is a pretty common idea for minimum response time. 
An algorithm called Shortest Remaining Processing Time (SRPT),
and its variants are often useful for minimizing metrics like mean
response time.

In \cite{bb20}, Berg et al study a related problem:
many heterogeneous tasks come in, some which are elastic and
exhibit perfect linear scaling of performance and some which are
inelastic which must be run on a single processor, according to
some stochastic assumptions, and they aim to minimize mean
response time. They show that for some parameter settings the
strategy \enquote{Inelastic First} is optimal.

In \cite{is16} Im et al exhibit an algorithm keeps the average
flow time small. 

In \cite{ga12} Gupta et al prove some impossibility results about
a problem somewhat similar to our problem.

Clearly related problems are widely studied.
Our problem is novel however, and interesting.

\subsection{Results}
\todo{copy abstract here and reword it}
